{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from os import path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_docname = \"Paper2-ElMo.pdf\"\n",
    "test_doc_path = path.join(\"test_docs\",test_docname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Deep contextualized word representations\\nMatthew E. Petersy, Mark Neumanny, Mohit Iyyery, Matt Gardnery,\\nfmatthewp,markn,mohiti,mattg g@allenai.org\\nChristopher Clark\\x03, Kenton Lee\\x03, Luke Zettlemoyery\\x03\\nfcsquared,kentonl,lsz g@cs.washington.edu',\n",
       " 'Christopher Clark\\x03, Kenton Lee\\x03, Luke Zettlemoyery\\x03\\nfcsquared,kentonl,lsz g@cs.washington.edu\\nyAllen Institute for Artiﬁcial Intelligence\\n\\x03Paul G. Allen School of Computer Science & Engineering, University of Washington\\nAbstract',\n",
       " '\\x03Paul G. Allen School of Computer Science & Engineering, University of Washington\\nAbstract\\nWe introduce a new type of deep contextual-\\nized word representation that models both (1)\\ncomplex characteristics of word use (e.g., syn-',\n",
       " 'ized word representation that models both (1)\\ncomplex characteristics of word use (e.g., syn-\\ntax and semantics), and (2) how these uses\\nvary across linguistic contexts (i.e., to model\\npolysemy). Our word vectors are learned func-',\n",
       " 'vary across linguistic contexts (i.e., to model\\npolysemy). Our word vectors are learned func-\\ntions of the internal states of a deep bidirec-\\ntional language model (biLM), which is pre-\\ntrained on a large text corpus. We show that',\n",
       " 'tional language model (biLM), which is pre-\\ntrained on a large text corpus. We show that\\nthese representations can be easily added to\\nexisting models and signiﬁcantly improve the\\nstate of the art across six challenging NLP',\n",
       " 'existing models and signiﬁcantly improve the\\nstate of the art across six challenging NLP\\nproblems, including question answering, tex-\\ntual entailment and sentiment analysis. We\\nalso present an analysis showing that exposing',\n",
       " 'tual entailment and sentiment analysis. We\\nalso present an analysis showing that exposing\\nthe deep internals of the pre-trained network is\\ncrucial, allowing downstream models to mix\\ndifferent types of semi-supervision signals.',\n",
       " 'crucial, allowing downstream models to mix\\ndifferent types of semi-supervision signals.\\n1 Introduction\\nPre-trained word representations (Mikolov et al.,\\n2013; Pennington et al., 2014) are a key compo-',\n",
       " 'Pre-trained word representations (Mikolov et al.,\\n2013; Pennington et al., 2014) are a key compo-\\nnent in many neural language understanding mod-\\nels. However, learning high quality representa-\\ntions can be challenging. They should ideally',\n",
       " 'els. However, learning high quality representa-\\ntions can be challenging. They should ideally\\nmodel both (1) complex characteristics of word\\nuse (e.g., syntax and semantics), and (2) how these\\nuses vary across linguistic contexts (i.e., to model',\n",
       " 'use (e.g., syntax and semantics), and (2) how these\\nuses vary across linguistic contexts (i.e., to model\\npolysemy). In this paper, we introduce a new type\\nofdeep contextualized word representation that\\ndirectly addresses both challenges, can be easily',\n",
       " 'ofdeep contextualized word representation that\\ndirectly addresses both challenges, can be easily\\nintegrated into existing models, and signiﬁcantly\\nimproves the state of the art in every considered\\ncase across a range of challenging language un-',\n",
       " 'improves the state of the art in every considered\\ncase across a range of challenging language un-\\nderstanding problems.\\nOur representations differ from traditional word\\ntype embeddings in that each token is assigned a',\n",
       " 'Our representations differ from traditional word\\ntype embeddings in that each token is assigned a\\nrepresentation that is a function of the entire input\\nsentence. We use vectors derived from a bidirec-\\ntional LSTM that is trained with a coupled lan-guage model (LM) objective on a large text cor-',\n",
       " 'sentence. We use vectors derived from a bidirec-\\ntional LSTM that is trained with a coupled lan-guage model (LM) objective on a large text cor-\\npus. For this reason, we call them ELMo (Em-\\nbeddings from Language Models) representations.\\nUnlike previous approaches for learning contextu-',\n",
       " 'beddings from Language Models) representations.\\nUnlike previous approaches for learning contextu-\\nalized word vectors (Peters et al., 2017; McCann\\net al., 2017), ELMo representations are deep, in\\nthe sense that they are a function of all of the in-',\n",
       " 'et al., 2017), ELMo representations are deep, in\\nthe sense that they are a function of all of the in-\\nternal layers of the biLM. More speciﬁcally, we\\nlearn a linear combination of the vectors stacked\\nabove each input word for each end task, which',\n",
       " 'learn a linear combination of the vectors stacked\\nabove each input word for each end task, which\\nmarkedly improves performance over just using\\nthe top LSTM layer.\\nCombining the internal states in this manner al-',\n",
       " 'the top LSTM layer.\\nCombining the internal states in this manner al-\\nlows for very rich word representations. Using in-\\ntrinsic evaluations, we show that the higher-level\\nLSTM states capture context-dependent aspects',\n",
       " 'trinsic evaluations, we show that the higher-level\\nLSTM states capture context-dependent aspects\\nof word meaning (e.g., they can be used with-\\nout modiﬁcation to perform well on supervised\\nword sense disambiguation tasks) while lower-',\n",
       " 'out modiﬁcation to perform well on supervised\\nword sense disambiguation tasks) while lower-\\nlevel states model aspects of syntax (e.g., they can\\nbe used to do part-of-speech tagging). Simultane-\\nously exposing all of these signals is highly bene-',\n",
       " 'be used to do part-of-speech tagging). Simultane-\\nously exposing all of these signals is highly bene-\\nﬁcial, allowing the learned models select the types\\nof semi-supervision that are most useful for each\\nend task.',\n",
       " 'of semi-supervision that are most useful for each\\nend task.\\nExtensive experiments demonstrate that ELMo\\nrepresentations work extremely well in practice.\\nWe ﬁrst show that they can be easily added to',\n",
       " 'representations work extremely well in practice.\\nWe ﬁrst show that they can be easily added to\\nexisting models for six diverse and challenging\\nlanguage understanding problems, including tex-\\ntual entailment, question answering and sentiment',\n",
       " 'language understanding problems, including tex-\\ntual entailment, question answering and sentiment\\nanalysis. The addition of ELMo representations\\nalone signiﬁcantly improves the state of the art\\nin every case, including up to 20% relative error',\n",
       " 'alone signiﬁcantly improves the state of the art\\nin every case, including up to 20% relative error\\nreductions. For tasks where direct comparisons\\nare possible, ELMo outperforms CoVe (McCann\\net al., 2017), which computes contextualized rep-',\n",
       " 'are possible, ELMo outperforms CoVe (McCann\\net al., 2017), which computes contextualized rep-\\nresentations using a neural machine translation en-\\ncoder. Finally, an analysis of both ELMo and\\nCoVe reveals that deep representations outperformarXiv:1802.05365v2  [cs.CL]  22 Mar 2018',\n",
       " 'coder. Finally, an analysis of both ELMo and\\nCoVe reveals that deep representations outperformarXiv:1802.05365v2  [cs.CL]  22 Mar 2018\\nthose derived from just the top layer of an LSTM.\\nOur trained models and code are publicly avail-\\nable, and we expect that ELMo will provide simi-',\n",
       " 'Our trained models and code are publicly avail-\\nable, and we expect that ELMo will provide simi-\\nlar gains for many other NLP problems.1\\n2 Related work\\nDue to their ability to capture syntactic and se-',\n",
       " '2 Related work\\nDue to their ability to capture syntactic and se-\\nmantic information of words from large scale un-\\nlabeled text, pretrained word vectors (Turian et al.,\\n2010; Mikolov et al., 2013; Pennington et al.,',\n",
       " 'labeled text, pretrained word vectors (Turian et al.,\\n2010; Mikolov et al., 2013; Pennington et al.,\\n2014) are a standard component of most state-of-\\nthe-art NLP architectures, including for question\\nanswering (Liu et al., 2017), textual entailment',\n",
       " 'the-art NLP architectures, including for question\\nanswering (Liu et al., 2017), textual entailment\\n(Chen et al., 2017) and semantic role labeling\\n(He et al., 2017). However, these approaches for\\nlearning word vectors only allow a single context-',\n",
       " '(He et al., 2017). However, these approaches for\\nlearning word vectors only allow a single context-\\nindependent representation for each word.\\nPreviously proposed methods overcome some\\nof the shortcomings of traditional word vectors',\n",
       " 'Previously proposed methods overcome some\\nof the shortcomings of traditional word vectors\\nby either enriching them with subword informa-\\ntion (e.g., Wieting et al., 2016; Bojanowski et al.,\\n2017) or learning separate vectors for each word',\n",
       " 'tion (e.g., Wieting et al., 2016; Bojanowski et al.,\\n2017) or learning separate vectors for each word\\nsense (e.g., Neelakantan et al., 2014). Our ap-\\nproach also beneﬁts from subword units through\\nthe use of character convolutions, and we seam-',\n",
       " 'proach also beneﬁts from subword units through\\nthe use of character convolutions, and we seam-\\nlessly incorporate multi-sense information into\\ndownstream tasks without explicitly training to\\npredict predeﬁned sense classes.',\n",
       " 'downstream tasks without explicitly training to\\npredict predeﬁned sense classes.\\nOther recent work has also focused on\\nlearning context-dependent representations.\\ncontext2vec (Melamud et al., 2016) uses a',\n",
       " 'learning context-dependent representations.\\ncontext2vec (Melamud et al., 2016) uses a\\nbidirectional Long Short Term Memory (LSTM;\\nHochreiter and Schmidhuber, 1997) to encode the\\ncontext around a pivot word. Other approaches',\n",
       " 'Hochreiter and Schmidhuber, 1997) to encode the\\ncontext around a pivot word. Other approaches\\nfor learning contextual embeddings include the\\npivot word itself in the representation and are\\ncomputed with the encoder of either a supervised',\n",
       " 'pivot word itself in the representation and are\\ncomputed with the encoder of either a supervised\\nneural machine translation (MT) system (CoVe;\\nMcCann et al., 2017) or an unsupervised lan-\\nguage model (Peters et al., 2017). Both of these',\n",
       " 'McCann et al., 2017) or an unsupervised lan-\\nguage model (Peters et al., 2017). Both of these\\napproaches beneﬁt from large datasets, although\\nthe MT approach is limited by the size of parallel\\ncorpora. In this paper, we take full advantage of',\n",
       " 'the MT approach is limited by the size of parallel\\ncorpora. In this paper, we take full advantage of\\naccess to plentiful monolingual data, and train\\nour biLM on a corpus with approximately 30\\nmillion sentences (Chelba et al., 2014). We also',\n",
       " 'our biLM on a corpus with approximately 30\\nmillion sentences (Chelba et al., 2014). We also\\ngeneralize these approaches to deep contextual\\nrepresentations, which we show work well across\\na broad range of diverse NLP tasks.',\n",
       " 'representations, which we show work well across\\na broad range of diverse NLP tasks.\\n1http://allennlp.org/elmoPrevious work has also shown that different lay-\\ners of deep biRNNs encode different types of in-\\nformation. For example, introducing multi-task',\n",
       " 'ers of deep biRNNs encode different types of in-\\nformation. For example, introducing multi-task\\nsyntactic supervision (e.g., part-of-speech tags) at\\nthe lower levels of a deep LSTM can improve\\noverall performance of higher level tasks such as',\n",
       " 'the lower levels of a deep LSTM can improve\\noverall performance of higher level tasks such as\\ndependency parsing (Hashimoto et al., 2017) or\\nCCG super tagging (Søgaard and Goldberg, 2016).\\nIn an RNN-based encoder-decoder machine trans-',\n",
       " 'CCG super tagging (Søgaard and Goldberg, 2016).\\nIn an RNN-based encoder-decoder machine trans-\\nlation system, Belinkov et al. (2017) showed that\\nthe representations learned at the ﬁrst layer in a 2-\\nlayer LSTM encoder are better at predicting POS',\n",
       " 'the representations learned at the ﬁrst layer in a 2-\\nlayer LSTM encoder are better at predicting POS\\ntags then second layer. Finally, the top layer of an\\nLSTM for encoding word context (Melamud et al.,\\n2016) has been shown to learn representations of',\n",
       " 'LSTM for encoding word context (Melamud et al.,\\n2016) has been shown to learn representations of\\nword sense. We show that similar signals are also\\ninduced by the modiﬁed language model objective\\nof our ELMo representations, and it can be very',\n",
       " 'induced by the modiﬁed language model objective\\nof our ELMo representations, and it can be very\\nbeneﬁcial to learn models for downstream tasks\\nthat mix these different types of semi-supervision.\\nDai and Le (2015) and Ramachandran et al.',\n",
       " 'that mix these different types of semi-supervision.\\nDai and Le (2015) and Ramachandran et al.\\n(2017) pretrain encoder-decoder pairs using lan-\\nguage models and sequence autoencoders and then\\nﬁne tune with task speciﬁc supervision. In con-',\n",
       " 'guage models and sequence autoencoders and then\\nﬁne tune with task speciﬁc supervision. In con-\\ntrast, after pretraining the biLM with unlabeled\\ndata, we ﬁx the weights and add additional task-\\nspeciﬁc model capacity, allowing us to leverage',\n",
       " 'data, we ﬁx the weights and add additional task-\\nspeciﬁc model capacity, allowing us to leverage\\nlarge, rich and universal biLM representations for\\ncases where downstream training data size dictates\\na smaller supervised model.',\n",
       " 'cases where downstream training data size dictates\\na smaller supervised model.\\n3 ELMo: Embeddings from Language\\nModels\\nUnlike most widely used word embeddings (Pen-',\n",
       " 'Models\\nUnlike most widely used word embeddings (Pen-\\nnington et al., 2014), ELMo word representations\\nare functions of the entire input sentence, as de-\\nscribed in this section. They are computed on top',\n",
       " 'are functions of the entire input sentence, as de-\\nscribed in this section. They are computed on top\\nof two-layer biLMs with character convolutions\\n(Sec. 3.1), as a linear function of the internal net-\\nwork states (Sec. 3.2). This setup allows us to do',\n",
       " '(Sec. 3.1), as a linear function of the internal net-\\nwork states (Sec. 3.2). This setup allows us to do\\nsemi-supervised learning, where the biLM is pre-\\ntrained at a large scale (Sec. 3.4) and easily incor-\\nporated into a wide range of existing neural NLP',\n",
       " 'trained at a large scale (Sec. 3.4) and easily incor-\\nporated into a wide range of existing neural NLP\\narchitectures (Sec. 3.3).\\n3.1 Bidirectional language models\\nGiven a sequence of Ntokens, (t1;t2;:::;t N), a',\n",
       " '3.1 Bidirectional language models\\nGiven a sequence of Ntokens, (t1;t2;:::;t N), a\\nforward language model computes the probability\\nof the sequence by modeling the probability of to-\\nkentkgiven the history (t1;:::;t k\\x001):',\n",
       " 'of the sequence by modeling the probability of to-\\nkentkgiven the history (t1;:::;t k\\x001):\\np(t1;t2;:::;t N) =NY\\nk=1p(tkjt1;t2;:::;t k\\x001):\\nRecent state-of-the-art neural language models',\n",
       " 'k=1p(tkjt1;t2;:::;t k\\x001):\\nRecent state-of-the-art neural language models\\n(J´ozefowicz et al., 2016; Melis et al., 2017; Mer-\\nity et al., 2017) compute a context-independent to-\\nken representation xLM',\n",
       " 'ity et al., 2017) compute a context-independent to-\\nken representation xLM\\nk(via token embeddings or\\na CNN over characters) then pass it through Llay-\\ners of forward LSTMs. At each position k, each',\n",
       " 'a CNN over characters) then pass it through Llay-\\ners of forward LSTMs. At each position k, each\\nLSTM layer outputs a context-dependent repre-\\nsentation\\x00 !hLM\\nk;jwherej= 1;:::;L . The top layer',\n",
       " 'sentation\\x00 !hLM\\nk;jwherej= 1;:::;L . The top layer\\nLSTM output,\\x00 !hLM\\nk;L, is used to predict the next\\ntokentk+1with a Softmax layer.',\n",
       " 'k;L, is used to predict the next\\ntokentk+1with a Softmax layer.\\nA backward LM is similar to a forward LM, ex-\\ncept it runs over the sequence in reverse, predict-\\ning the previous token given the future context:',\n",
       " 'cept it runs over the sequence in reverse, predict-\\ning the previous token given the future context:\\np(t1;t2;:::;t N) =NY\\nk=1p(tkjtk+1;tk+2;:::;t N):\\nIt can be implemented in an analogous way to a',\n",
       " 'k=1p(tkjtk+1;tk+2;:::;t N):\\nIt can be implemented in an analogous way to a\\nforward LM, with each backward LSTM layer j\\nin aLlayer deep model producing representations \\x00hLM\\nk;joftkgiven (tk+1;:::;t N).',\n",
       " 'in aLlayer deep model producing representations \\x00hLM\\nk;joftkgiven (tk+1;:::;t N).\\nA biLM combines both a forward and backward\\nLM. Our formulation jointly maximizes the log\\nlikelihood of the forward and backward directions:',\n",
       " 'LM. Our formulation jointly maximizes the log\\nlikelihood of the forward and backward directions:\\nNX\\nk=1( logp(tkjt1;:::;t k\\x001; \\x02x;\\x00 !\\x02LSTM;\\x02s)\\n+ logp(tkjtk+1;:::;t N; \\x02x; \\x00\\x02LSTM;\\x02s) ):',\n",
       " 'k=1( logp(tkjt1;:::;t k\\x001; \\x02x;\\x00 !\\x02LSTM;\\x02s)\\n+ logp(tkjtk+1;:::;t N; \\x02x; \\x00\\x02LSTM;\\x02s) ):\\nWe tie the parameters for both the token represen-\\ntation ( \\x02x) and Softmax layer ( \\x02s) in the forward\\nand backward direction while maintaining sepa-',\n",
       " 'tation ( \\x02x) and Softmax layer ( \\x02s) in the forward\\nand backward direction while maintaining sepa-\\nrate parameters for the LSTMs in each direction.\\nOverall, this formulation is similar to the approach\\nof Peters et al. (2017), with the exception that we',\n",
       " 'Overall, this formulation is similar to the approach\\nof Peters et al. (2017), with the exception that we\\nshare some weights between directions instead of\\nusing completely independent parameters. In the\\nnext section, we depart from previous work by in-',\n",
       " 'using completely independent parameters. In the\\nnext section, we depart from previous work by in-\\ntroducing a new approach for learning word rep-\\nresentations that are a linear combination of the\\nbiLM layers.',\n",
       " 'resentations that are a linear combination of the\\nbiLM layers.\\n3.2 ELMo\\nELMo is a task speciﬁc combination of the in-\\ntermediate layer representations in the biLM. Foreach tokentk, aL-layer biLM computes a set of',\n",
       " 'ELMo is a task speciﬁc combination of the in-\\ntermediate layer representations in the biLM. Foreach tokentk, aL-layer biLM computes a set of\\n2L+ 1representations\\nRk=fxLM\\nk;\\x00 !hLM',\n",
       " 'Rk=fxLM\\nk;\\x00 !hLM\\nk;j; \\x00hLM\\nk;jjj= 1;:::;Lg\\n=fhLM',\n",
       " 'k;jjj= 1;:::;Lg\\n=fhLM\\nk;jjj= 0;:::;Lg;\\nwhere hLM\\nk;0is the token layer and hLM',\n",
       " 'where hLM\\nk;0is the token layer and hLM\\nk;j=\\n[\\x00 !hLM\\nk;j; \\x00hLM',\n",
       " '[\\x00 !hLM\\nk;j; \\x00hLM\\nk;j], for each biLSTM layer.\\nFor inclusion in a downstream model, ELMo\\ncollapses all layers in Rinto a single vector,',\n",
       " 'For inclusion in a downstream model, ELMo\\ncollapses all layers in Rinto a single vector,\\nELMo k=E(Rk;\\x02e). In the simplest case,\\nELMo just selects the top layer, E(Rk) =hLM\\nk;L,',\n",
       " 'ELMo just selects the top layer, E(Rk) =hLM\\nk;L,\\nas in TagLM (Peters et al., 2017) and CoVe (Mc-\\nCann et al., 2017). More generally, we compute a\\ntask speciﬁc weighting of all biLM layers:',\n",
       " 'Cann et al., 2017). More generally, we compute a\\ntask speciﬁc weighting of all biLM layers:\\nELMotask\\nk=E(Rk; \\x02task) =\\rtaskLX\\nj=0stask',\n",
       " 'k=E(Rk; \\x02task) =\\rtaskLX\\nj=0stask\\njhLM\\nk;j:\\n(1)',\n",
       " 'k;j:\\n(1)\\nIn (1), staskare softmax-normalized weights and\\nthe scalar parameter \\rtaskallows the task model to\\nscale the entire ELMo vector. \\ris of practical im-',\n",
       " 'the scalar parameter \\rtaskallows the task model to\\nscale the entire ELMo vector. \\ris of practical im-\\nportance to aid the optimization process (see sup-\\nplemental material for details). Considering that\\nthe activations of each biLM layer have a different',\n",
       " 'plemental material for details). Considering that\\nthe activations of each biLM layer have a different\\ndistribution, in some cases it also helped to apply\\nlayer normalization (Ba et al., 2016) to each biLM\\nlayer before weighting.',\n",
       " 'layer normalization (Ba et al., 2016) to each biLM\\nlayer before weighting.\\n3.3 Using biLMs for supervised NLP tasks\\nGiven a pre-trained biLM and a supervised archi-\\ntecture for a target NLP task, it is a simple process',\n",
       " 'Given a pre-trained biLM and a supervised archi-\\ntecture for a target NLP task, it is a simple process\\nto use the biLM to improve the task model. We\\nsimply run the biLM and record all of the layer\\nrepresentations for each word. Then, we let the',\n",
       " 'simply run the biLM and record all of the layer\\nrepresentations for each word. Then, we let the\\nend task model learn a linear combination of these\\nrepresentations, as described below.\\nFirst consider the lowest layers of the super-',\n",
       " 'representations, as described below.\\nFirst consider the lowest layers of the super-\\nvised model without the biLM. Most supervised\\nNLP models share a common architecture at the\\nlowest layers, allowing us to add ELMo in a',\n",
       " 'NLP models share a common architecture at the\\nlowest layers, allowing us to add ELMo in a\\nconsistent, uniﬁed manner. Given a sequence\\nof tokens (t1;:::;t N), it is standard to form a\\ncontext-independent token representation xkfor',\n",
       " 'of tokens (t1;:::;t N), it is standard to form a\\ncontext-independent token representation xkfor\\neach token position using pre-trained word em-\\nbeddings and optionally character-based represen-\\ntations. Then, the model forms a context-sensitive',\n",
       " 'beddings and optionally character-based represen-\\ntations. Then, the model forms a context-sensitive\\nrepresentation hk, typically using either bidirec-\\ntional RNNs, CNNs, or feed forward networks.\\nTo add ELMo to the supervised model, we',\n",
       " 'tional RNNs, CNNs, or feed forward networks.\\nTo add ELMo to the supervised model, we\\nﬁrst freeze the weights of the biLM and then\\nconcatenate the ELMo vector ELMotask\\nk with',\n",
       " 'concatenate the ELMo vector ELMotask\\nk with\\nxkand pass the ELMo enhanced representation\\n[xk;ELMotask\\nk]into the task RNN. For some',\n",
       " '[xk;ELMotask\\nk]into the task RNN. For some\\ntasks (e.g., SNLI, SQuAD), we observe further\\nimprovements by also including ELMo at the out-\\nput of the task RNN by introducing another set',\n",
       " 'improvements by also including ELMo at the out-\\nput of the task RNN by introducing another set\\nof output speciﬁc linear weights and replacing hk\\nwith [hk;ELMotask\\nk]. As the remainder of the',\n",
       " 'with [hk;ELMotask\\nk]. As the remainder of the\\nsupervised model remains unchanged, these addi-\\ntions can happen within the context of more com-\\nplex neural models. For example, see the SNLI',\n",
       " 'tions can happen within the context of more com-\\nplex neural models. For example, see the SNLI\\nexperiments in Sec. 4 where a bi-attention layer\\nfollows the biLSTMs, or the coreference resolu-\\ntion experiments where a clustering model is lay-',\n",
       " 'follows the biLSTMs, or the coreference resolu-\\ntion experiments where a clustering model is lay-\\nered on top of the biLSTMs.\\nFinally, we found it beneﬁcial to add a moder-\\nate amount of dropout to ELMo (Srivastava et al.,',\n",
       " 'Finally, we found it beneﬁcial to add a moder-\\nate amount of dropout to ELMo (Srivastava et al.,\\n2014) and in some cases to regularize the ELMo\\nweights by adding \\x15kwk2\\n2to the loss. This im-',\n",
       " 'weights by adding \\x15kwk2\\n2to the loss. This im-\\nposes an inductive bias on the ELMo weights to\\nstay close to an average of all biLM layers.\\n3.4 Pre-trained bidirectional language model',\n",
       " 'stay close to an average of all biLM layers.\\n3.4 Pre-trained bidirectional language model\\narchitecture\\nThe pre-trained biLMs in this paper are similar to\\nthe architectures in J ´ozefowicz et al. (2016) and',\n",
       " 'The pre-trained biLMs in this paper are similar to\\nthe architectures in J ´ozefowicz et al. (2016) and\\nKim et al. (2015), but modiﬁed to support joint\\ntraining of both directions and add a residual con-\\nnection between LSTM layers. We focus on large',\n",
       " 'training of both directions and add a residual con-\\nnection between LSTM layers. We focus on large\\nscale biLMs in this work, as Peters et al. (2017)\\nhighlighted the importance of using biLMs over\\nforward-only LMs and large scale training.',\n",
       " 'highlighted the importance of using biLMs over\\nforward-only LMs and large scale training.\\nTo balance overall language model perplexity\\nwith model size and computational requirements\\nfor downstream tasks while maintaining a purely',\n",
       " 'with model size and computational requirements\\nfor downstream tasks while maintaining a purely\\ncharacter-based input representation, we halved all\\nembedding and hidden dimensions from the single\\nbest model CNN-BIG-LSTM in J´ozefowicz et al.',\n",
       " 'embedding and hidden dimensions from the single\\nbest model CNN-BIG-LSTM in J´ozefowicz et al.\\n(2016). The ﬁnal model uses L= 2biLSTM lay-\\ners with 4096 units and 512 dimension projections\\nand a residual connection from the ﬁrst to second',\n",
       " 'ers with 4096 units and 512 dimension projections\\nand a residual connection from the ﬁrst to second\\nlayer. The context insensitive type representation\\nuses 2048 character n-gram convolutional ﬁlters\\nfollowed by two highway layers (Srivastava et al.,',\n",
       " 'uses 2048 character n-gram convolutional ﬁlters\\nfollowed by two highway layers (Srivastava et al.,\\n2015) and a linear projection down to a 512 repre-\\nsentation. As a result, the biLM provides three lay-\\ners of representations for each input token, includ-',\n",
       " 'sentation. As a result, the biLM provides three lay-\\ners of representations for each input token, includ-\\ning those outside the training set due to the purely\\ncharacter input. In contrast, traditional word em-\\nbedding methods only provide one layer of repre-',\n",
       " 'character input. In contrast, traditional word em-\\nbedding methods only provide one layer of repre-\\nsentation for tokens in a ﬁxed vocabulary.After training for 10 epochs on the 1B Word\\nBenchmark (Chelba et al., 2014), the average for-\\nward and backward perplexities is 39.7, compared',\n",
       " 'Benchmark (Chelba et al., 2014), the average for-\\nward and backward perplexities is 39.7, compared\\nto 30.0 for the forward CNN-BIG-LSTM . Gener-\\nally, we found the forward and backward perplex-\\nities to be approximately equal, with the backward',\n",
       " 'ally, we found the forward and backward perplex-\\nities to be approximately equal, with the backward\\nvalue slightly lower.\\nOnce pretrained, the biLM can compute repre-\\nsentations for any task. In some cases, ﬁne tuning',\n",
       " 'Once pretrained, the biLM can compute repre-\\nsentations for any task. In some cases, ﬁne tuning\\nthe biLM on domain speciﬁc data leads to signiﬁ-\\ncant drops in perplexity and an increase in down-\\nstream task performance. This can be seen as a',\n",
       " 'cant drops in perplexity and an increase in down-\\nstream task performance. This can be seen as a\\ntype of domain transfer for the biLM. As a result,\\nin most cases we used a ﬁne-tuned biLM in the\\ndownstream task. See supplemental material for',\n",
       " 'in most cases we used a ﬁne-tuned biLM in the\\ndownstream task. See supplemental material for\\ndetails.\\n4 Evaluation\\nTable 1 shows the performance of ELMo across a',\n",
       " '4 Evaluation\\nTable 1 shows the performance of ELMo across a\\ndiverse set of six benchmark NLP tasks. In every\\ntask considered, simply adding ELMo establishes\\na new state-of-the-art result, with relative error re-',\n",
       " 'task considered, simply adding ELMo establishes\\na new state-of-the-art result, with relative error re-\\nductions ranging from 6 - 20% over strong base\\nmodels. This is a very general result across a di-\\nverse set model architectures and language under-',\n",
       " 'models. This is a very general result across a di-\\nverse set model architectures and language under-\\nstanding tasks. In the remainder of this section we\\nprovide high-level sketches of the individual task\\nresults; see the supplemental material for full ex-',\n",
       " 'provide high-level sketches of the individual task\\nresults; see the supplemental material for full ex-\\nperimental details.\\nQuestion answering The Stanford Question\\nAnswering Dataset (SQuAD) (Rajpurkar et al.,',\n",
       " 'Question answering The Stanford Question\\nAnswering Dataset (SQuAD) (Rajpurkar et al.,\\n2016) contains 100K+ crowd sourced question-\\nanswer pairs where the answer is a span in a given\\nWikipedia paragraph. Our baseline model (Clark',\n",
       " 'answer pairs where the answer is a span in a given\\nWikipedia paragraph. Our baseline model (Clark\\nand Gardner, 2017) is an improved version of the\\nBidirectional Attention Flow model in Seo et al.\\n(BiDAF; 2017). It adds a self-attention layer af-',\n",
       " 'Bidirectional Attention Flow model in Seo et al.\\n(BiDAF; 2017). It adds a self-attention layer af-\\nter the bidirectional attention component, simpli-\\nﬁes some of the pooling operations and substitutes\\nthe LSTMs for gated recurrent units (GRUs; Cho',\n",
       " 'ﬁes some of the pooling operations and substitutes\\nthe LSTMs for gated recurrent units (GRUs; Cho\\net al., 2014). After adding ELMo to the baseline\\nmodel, test set F 1improved by 4.7% from 81.1%\\nto 85.8%, a 24.9% relative error reduction over the',\n",
       " 'model, test set F 1improved by 4.7% from 81.1%\\nto 85.8%, a 24.9% relative error reduction over the\\nbaseline, and improving the overall single model\\nstate-of-the-art by 1.4%. A 11 member ensem-\\nble pushes F 1to 87.4, the overall state-of-the-art',\n",
       " 'state-of-the-art by 1.4%. A 11 member ensem-\\nble pushes F 1to 87.4, the overall state-of-the-art\\nat time of submission to the leaderboard.2The\\nincrease of 4.7% with ELMo is also signiﬁcantly\\nlarger then the 1.8% improvement from adding',\n",
       " 'increase of 4.7% with ELMo is also signiﬁcantly\\nlarger then the 1.8% improvement from adding\\nCoVe to a baseline model (McCann et al., 2017).\\n2As of November 17, 2017.\\nTASK PREVIOUS SOTAOUR',\n",
       " '2As of November 17, 2017.\\nTASK PREVIOUS SOTAOUR\\nBASELINEELM O+\\nBASELINEINCREASE\\n(ABSOLUTE /',\n",
       " 'BASELINEINCREASE\\n(ABSOLUTE /\\nRELATIVE )\\nSQuAD Liu et al. (2017) 84.4 81.1 85.8 4.7 / 24.9%\\nSNLI Chen et al. (2017) 88.6 88.0 88.7\\x060.17 0.7 / 5.8%',\n",
       " 'SQuAD Liu et al. (2017) 84.4 81.1 85.8 4.7 / 24.9%\\nSNLI Chen et al. (2017) 88.6 88.0 88.7\\x060.17 0.7 / 5.8%\\nSRL He et al. (2017) 81.7 81.4 84.6 3.2 / 17.2%\\nCoref Lee et al. (2017) 67.2 67.2 70.4 3.2 / 9.8%\\nNER Peters et al. (2017) 91.93 \\x060.19 90.15 92.22\\x060.10 2.06 / 21%',\n",
       " 'Coref Lee et al. (2017) 67.2 67.2 70.4 3.2 / 9.8%\\nNER Peters et al. (2017) 91.93 \\x060.19 90.15 92.22\\x060.10 2.06 / 21%\\nSST-5 McCann et al. (2017) 53.7 51.4 54.7\\x060.5 3.3 / 6.8%\\nTable 1: Test set comparison of ELMo enhanced neural models with state-of-the-art single model baselines across\\nsix benchmark NLP tasks. The performance metric varies across tasks – accuracy for SNLI and SST-5; F 1for',\n",
       " 'Table 1: Test set comparison of ELMo enhanced neural models with state-of-the-art single model baselines across\\nsix benchmark NLP tasks. The performance metric varies across tasks – accuracy for SNLI and SST-5; F 1for\\nSQuAD, SRL and NER; average F 1for Coref. Due to the small test sizes for NER and SST-5, we report the mean\\nand standard deviation across ﬁve runs with different random seeds. The “increase” column lists both the absolute\\nand relative improvements over our baseline.',\n",
       " 'and standard deviation across ﬁve runs with different random seeds. The “increase” column lists both the absolute\\nand relative improvements over our baseline.\\nTextual entailment Textual entailment is the\\ntask of determining whether a “hypothesis” is\\ntrue, given a “premise”. The Stanford Natu-',\n",
       " 'task of determining whether a “hypothesis” is\\ntrue, given a “premise”. The Stanford Natu-\\nral Language Inference (SNLI) corpus (Bowman\\net al., 2015) provides approximately 550K hypoth-\\nesis/premise pairs. Our baseline, the ESIM se-',\n",
       " 'et al., 2015) provides approximately 550K hypoth-\\nesis/premise pairs. Our baseline, the ESIM se-\\nquence model from Chen et al. (2017), uses a biL-\\nSTM to encode the premise and hypothesis, fol-\\nlowed by a matrix attention layer, a local infer-',\n",
       " 'STM to encode the premise and hypothesis, fol-\\nlowed by a matrix attention layer, a local infer-\\nence layer, another biLSTM inference composi-\\ntion layer, and ﬁnally a pooling operation before\\nthe output layer. Overall, adding ELMo to the',\n",
       " 'tion layer, and ﬁnally a pooling operation before\\nthe output layer. Overall, adding ELMo to the\\nESIM model improves accuracy by an average of\\n0.7% across ﬁve random seeds. A ﬁve member\\nensemble pushes the overall accuracy to 89.3%,',\n",
       " '0.7% across ﬁve random seeds. A ﬁve member\\nensemble pushes the overall accuracy to 89.3%,\\nexceeding the previous ensemble best of 88.9%\\n(Gong et al., 2018).\\nSemantic role labeling A semantic role label-',\n",
       " '(Gong et al., 2018).\\nSemantic role labeling A semantic role label-\\ning (SRL) system models the predicate-argument\\nstructure of a sentence, and is often described as\\nanswering “Who did what to whom”. He et al.',\n",
       " 'structure of a sentence, and is often described as\\nanswering “Who did what to whom”. He et al.\\n(2017) modeled SRL as a BIO tagging problem\\nand used an 8-layer deep biLSTM with forward\\nand backward directions interleaved, following',\n",
       " 'and used an 8-layer deep biLSTM with forward\\nand backward directions interleaved, following\\nZhou and Xu (2015). As shown in Table 1, when\\nadding ELMo to a re-implementation of He et al.\\n(2017) the single model test set F 1jumped 3.2%',\n",
       " 'adding ELMo to a re-implementation of He et al.\\n(2017) the single model test set F 1jumped 3.2%\\nfrom 81.4% to 84.6% – a new state-of-the-art on\\nthe OntoNotes benchmark (Pradhan et al., 2013),\\neven improving over the previous best ensemble',\n",
       " 'the OntoNotes benchmark (Pradhan et al., 2013),\\neven improving over the previous best ensemble\\nresult by 1.2%.\\nCoreference resolution Coreference resolution\\nis the task of clustering mentions in text that re-',\n",
       " 'Coreference resolution Coreference resolution\\nis the task of clustering mentions in text that re-\\nfer to the same underlying real world entities. Our\\nbaseline model is the end-to-end span-based neu-\\nral model of Lee et al. (2017). It uses a biLSTMand attention mechanism to ﬁrst compute span',\n",
       " 'baseline model is the end-to-end span-based neu-\\nral model of Lee et al. (2017). It uses a biLSTMand attention mechanism to ﬁrst compute span\\nrepresentations and then applies a softmax men-\\ntion ranking model to ﬁnd coreference chains. In\\nour experiments with the OntoNotes coreference',\n",
       " 'tion ranking model to ﬁnd coreference chains. In\\nour experiments with the OntoNotes coreference\\nannotations from the CoNLL 2012 shared task\\n(Pradhan et al., 2012), adding ELMo improved the\\naverage F 1by 3.2% from 67.2 to 70.4, establish-',\n",
       " '(Pradhan et al., 2012), adding ELMo improved the\\naverage F 1by 3.2% from 67.2 to 70.4, establish-\\ning a new state of the art, again improving over the\\nprevious best ensemble result by 1.6% F 1.\\nNamed entity extraction The CoNLL 2003',\n",
       " 'previous best ensemble result by 1.6% F 1.\\nNamed entity extraction The CoNLL 2003\\nNER task (Sang and Meulder, 2003) consists of\\nnewswire from the Reuters RCV1 corpus tagged\\nwith four different entity types ( PER,LOC,ORG,',\n",
       " 'newswire from the Reuters RCV1 corpus tagged\\nwith four different entity types ( PER,LOC,ORG,\\nMISC ). Following recent state-of-the-art systems\\n(Lample et al., 2016; Peters et al., 2017), the base-\\nline model uses pre-trained word embeddings, a',\n",
       " '(Lample et al., 2016; Peters et al., 2017), the base-\\nline model uses pre-trained word embeddings, a\\ncharacter-based CNN representation, two biLSTM\\nlayers and a conditional random ﬁeld (CRF) loss\\n(Lafferty et al., 2001), similar to Collobert et al.',\n",
       " 'layers and a conditional random ﬁeld (CRF) loss\\n(Lafferty et al., 2001), similar to Collobert et al.\\n(2011). As shown in Table 1, our ELMo enhanced\\nbiLSTM-CRF achieves 92.22% F 1averaged over\\nﬁve runs. The key difference between our system',\n",
       " 'biLSTM-CRF achieves 92.22% F 1averaged over\\nﬁve runs. The key difference between our system\\nand the previous state of the art from Peters et al.\\n(2017) is that we allowed the task model to learn a\\nweighted average of all biLM layers, whereas Pe-',\n",
       " '(2017) is that we allowed the task model to learn a\\nweighted average of all biLM layers, whereas Pe-\\nters et al. (2017) only use the top biLM layer. As\\nshown in Sec. 5.1, using all layers instead of just\\nthe last layer improves performance across multi-',\n",
       " 'shown in Sec. 5.1, using all layers instead of just\\nthe last layer improves performance across multi-\\nple tasks.\\nSentiment analysis The ﬁne-grained sentiment\\nclassiﬁcation task in the Stanford Sentiment Tree-',\n",
       " 'Sentiment analysis The ﬁne-grained sentiment\\nclassiﬁcation task in the Stanford Sentiment Tree-\\nbank (SST-5; Socher et al., 2013) involves select-\\ning one of ﬁve labels (from very negative to very\\npositive) to describe a sentence from a movie re-',\n",
       " 'ing one of ﬁve labels (from very negative to very\\npositive) to describe a sentence from a movie re-\\nview. The sentences contain diverse linguistic\\nphenomena such as idioms and complex syntac-\\nTask Baseline Last OnlyAll layers',\n",
       " 'phenomena such as idioms and complex syntac-\\nTask Baseline Last OnlyAll layers\\n\\x15=1\\x15=0.001\\nSQuAD 80.8 84.7 85.0 85.2\\nSNLI 88.1 89.1 89.3 89.5',\n",
       " 'SQuAD 80.8 84.7 85.0 85.2\\nSNLI 88.1 89.1 89.3 89.5\\nSRL 81.6 84.1 84.6 84.8\\nTable 2: Development set performance for SQuAD,\\nSNLI and SRL comparing using all layers of the biLM',\n",
       " 'Table 2: Development set performance for SQuAD,\\nSNLI and SRL comparing using all layers of the biLM\\n(with different choices of regularization strength \\x15) to\\njust the top layer.\\nTaskInput',\n",
       " 'just the top layer.\\nTaskInput\\nOnlyInput &\\nOutputOutput\\nOnly',\n",
       " 'OutputOutput\\nOnly\\nSQuAD 85.1 85.6 84.8\\nSNLI 88.9 89.5 88.7\\nSRL 84.7 84.3 80.9',\n",
       " 'SNLI 88.9 89.5 88.7\\nSRL 84.7 84.3 80.9\\nTable 3: Development set performance for SQuAD,\\nSNLI and SRL when including ELMo at different lo-\\ncations in the supervised model.',\n",
       " 'SNLI and SRL when including ELMo at different lo-\\ncations in the supervised model.\\ntic constructions such as negations that are difﬁ-\\ncult for models to learn. Our baseline model is\\nthe biattentive classiﬁcation network (BCN) from',\n",
       " 'cult for models to learn. Our baseline model is\\nthe biattentive classiﬁcation network (BCN) from\\nMcCann et al. (2017), which also held the prior\\nstate-of-the-art result when augmented with CoVe\\nembeddings. Replacing CoVe with ELMo in the',\n",
       " 'state-of-the-art result when augmented with CoVe\\nembeddings. Replacing CoVe with ELMo in the\\nBCN model results in a 1.0% absolute accuracy\\nimprovement over the state of the art.\\n5 Analysis',\n",
       " 'improvement over the state of the art.\\n5 Analysis\\nThis section provides an ablation analysis to vali-\\ndate our chief claims and to elucidate some inter-\\nesting aspects of ELMo representations. Sec. 5.1',\n",
       " 'date our chief claims and to elucidate some inter-\\nesting aspects of ELMo representations. Sec. 5.1\\nshows that using deep contextual representations\\nin downstream tasks improves performance over\\nprevious work that uses just the top layer, regard-',\n",
       " 'in downstream tasks improves performance over\\nprevious work that uses just the top layer, regard-\\nless of whether they are produced from a biLM or\\nMT encoder, and that ELMo representations pro-\\nvide the best overall performance. Sec. 5.3 ex-',\n",
       " 'MT encoder, and that ELMo representations pro-\\nvide the best overall performance. Sec. 5.3 ex-\\nplores the different types of contextual informa-\\ntion captured in biLMs and uses two intrinsic eval-\\nuations to show that syntactic information is better',\n",
       " 'tion captured in biLMs and uses two intrinsic eval-\\nuations to show that syntactic information is better\\nrepresented at lower layers while semantic infor-\\nmation is captured a higher layers, consistent with\\nMT encoders. It also shows that our biLM consis-',\n",
       " 'mation is captured a higher layers, consistent with\\nMT encoders. It also shows that our biLM consis-\\ntently provides richer representations then CoVe.\\nAdditionally, we analyze the sensitivity to where\\nELMo is included in the task model (Sec. 5.2),',\n",
       " 'Additionally, we analyze the sensitivity to where\\nELMo is included in the task model (Sec. 5.2),\\ntraining set size (Sec. 5.4), and visualize the ELMo\\nlearned weights across the tasks (Sec. 5.5).5.1 Alternate layer weighting schemes\\nThere are many alternatives to Equation 1 for com-',\n",
       " 'learned weights across the tasks (Sec. 5.5).5.1 Alternate layer weighting schemes\\nThere are many alternatives to Equation 1 for com-\\nbining the biLM layers. Previous work on con-\\ntextual representations used only the last layer,\\nwhether it be from a biLM (Peters et al., 2017) or',\n",
       " 'textual representations used only the last layer,\\nwhether it be from a biLM (Peters et al., 2017) or\\nan MT encoder (CoVe; McCann et al., 2017). The\\nchoice of the regularization parameter \\x15is also\\nimportant, as large values such as \\x15= 1 effec-',\n",
       " 'choice of the regularization parameter \\x15is also\\nimportant, as large values such as \\x15= 1 effec-\\ntively reduce the weighting function to a simple\\naverage over the layers, while smaller values (e.g.,\\n\\x15= 0:001) allow the layer weights to vary.',\n",
       " 'average over the layers, while smaller values (e.g.,\\n\\x15= 0:001) allow the layer weights to vary.\\nTable 2 compares these alternatives for SQuAD,\\nSNLI and SRL. Including representations from all\\nlayers improves overall performance over just us-',\n",
       " 'SNLI and SRL. Including representations from all\\nlayers improves overall performance over just us-\\ning the last layer, and including contextual rep-\\nresentations from the last layer improves perfor-\\nmance over the baseline. For example, in the',\n",
       " 'resentations from the last layer improves perfor-\\nmance over the baseline. For example, in the\\ncase of SQuAD, using just the last biLM layer im-\\nproves development F 1by 3.9% over the baseline.\\nAveraging all biLM layers instead of using just the',\n",
       " 'proves development F 1by 3.9% over the baseline.\\nAveraging all biLM layers instead of using just the\\nlast layer improves F 1another 0.3% (comparing\\n“Last Only” to \\x15=1 columns), and allowing the\\ntask model to learn individual layer weights im-',\n",
       " '“Last Only” to \\x15=1 columns), and allowing the\\ntask model to learn individual layer weights im-\\nproves F 1another 0.2% ( \\x15=1 vs.\\x15=0.001). A\\nsmall\\x15is preferred in most cases with ELMo, al-\\nthough for NER, a task with a smaller training set,',\n",
       " 'small\\x15is preferred in most cases with ELMo, al-\\nthough for NER, a task with a smaller training set,\\nthe results are insensitive to \\x15(not shown).\\nThe overall trend is similar with CoVe but with\\nsmaller increases over the baseline. For SNLI, av-',\n",
       " 'The overall trend is similar with CoVe but with\\nsmaller increases over the baseline. For SNLI, av-\\neraging all layers with \\x15=1 improves development\\naccuracy from 88.2 to 88.7% over using just the\\nlast layer. SRL F 1increased a marginal 0.1% to',\n",
       " 'accuracy from 88.2 to 88.7% over using just the\\nlast layer. SRL F 1increased a marginal 0.1% to\\n82.2 for the \\x15=1 case compared to using the last\\nlayer only.\\n5.2 Where to include ELMo?',\n",
       " 'layer only.\\n5.2 Where to include ELMo?\\nAll of the task architectures in this paper include\\nword embeddings only as input to the lowest layer\\nbiRNN. However, we ﬁnd that including ELMo at',\n",
       " 'word embeddings only as input to the lowest layer\\nbiRNN. However, we ﬁnd that including ELMo at\\nthe output of the biRNN in task-speciﬁc architec-\\ntures improves overall results for some tasks. As\\nshown in Table 3, including ELMo at both the in-',\n",
       " 'tures improves overall results for some tasks. As\\nshown in Table 3, including ELMo at both the in-\\nput and output layers for SNLI and SQuAD im-\\nproves over just the input layer, but for SRL (and\\ncoreference resolution, not shown) performance is',\n",
       " 'proves over just the input layer, but for SRL (and\\ncoreference resolution, not shown) performance is\\nhighest when it is included at just the input layer.\\nOne possible explanation for this result is that both\\nthe SNLI and SQuAD architectures use attention',\n",
       " 'One possible explanation for this result is that both\\nthe SNLI and SQuAD architectures use attention\\nlayers after the biRNN, so introducing ELMo at\\nthis layer allows the model to attend directly to the\\nbiLM’s internal representations. In the SRL case,',\n",
       " 'this layer allows the model to attend directly to the\\nbiLM’s internal representations. In the SRL case,\\nSource Nearest Neighbors\\nGloVe playplaying, game, games, played, players, plays, player,\\nPlay, football, multiplayer',\n",
       " 'GloVe playplaying, game, games, played, players, plays, player,\\nPlay, football, multiplayer\\nbiLMChico Ruiz made a spec-\\ntacular play on Alusik ’s\\ngrounderf. . .gKieffer , the only junior in the group , was commended',\n",
       " 'tacular play on Alusik ’s\\ngrounderf. . .gKieffer , the only junior in the group , was commended\\nfor his ability to hit in the clutch , as well as his all-round\\nexcellent play .\\nOlivia De Havilland',\n",
       " 'excellent play .\\nOlivia De Havilland\\nsigned to do a Broadway\\nplay for Garsonf. . .gf. . .gthey were actors who had been handed fat roles in\\na successful play , and had talent enough to ﬁll the roles',\n",
       " 'play for Garsonf. . .gf. . .gthey were actors who had been handed fat roles in\\na successful play , and had talent enough to ﬁll the roles\\ncompetently , with nice understatement .\\nTable 4: Nearest neighbors to “play” using GloVe and the context embeddings from a biLM.\\nModel F1',\n",
       " 'Table 4: Nearest neighbors to “play” using GloVe and the context embeddings from a biLM.\\nModel F1\\nWordNet 1st Sense Baseline 65.9\\nRaganato et al. (2017a) 69.9\\nIacobacci et al. (2016) 70.1',\n",
       " 'Raganato et al. (2017a) 69.9\\nIacobacci et al. (2016) 70.1\\nCoVe, First Layer 59.4\\nCoVe, Second Layer 64.7\\nbiLM, First layer 67.4',\n",
       " 'CoVe, Second Layer 64.7\\nbiLM, First layer 67.4\\nbiLM, Second layer 69.0\\nTable 5: All-words ﬁne grained WSD F 1. For CoVe\\nand the biLM, we report scores for both the ﬁrst and',\n",
       " 'Table 5: All-words ﬁne grained WSD F 1. For CoVe\\nand the biLM, we report scores for both the ﬁrst and\\nsecond layer biLSTMs.\\nthe task-speciﬁc context representations are likely\\nmore important than those from the biLM.',\n",
       " 'the task-speciﬁc context representations are likely\\nmore important than those from the biLM.\\n5.3 What information is captured by the\\nbiLM’s representations?\\nSince adding ELMo improves task performance',\n",
       " 'biLM’s representations?\\nSince adding ELMo improves task performance\\nover word vectors alone, the biLM’s contextual\\nrepresentations must encode information gener-\\nally useful for NLP tasks that is not captured',\n",
       " 'representations must encode information gener-\\nally useful for NLP tasks that is not captured\\nin word vectors. Intuitively, the biLM must\\nbe disambiguating the meaning of words using\\ntheir context. Consider “play”, a highly poly-',\n",
       " 'be disambiguating the meaning of words using\\ntheir context. Consider “play”, a highly poly-\\nsemous word. The top of Table 4 lists near-\\nest neighbors to “play” using GloVe vectors.\\nThey are spread across several parts of speech',\n",
       " 'est neighbors to “play” using GloVe vectors.\\nThey are spread across several parts of speech\\n(e.g., “played”, “playing” as verbs, and “player”,\\n“game” as nouns) but concentrated in the sports-\\nrelated senses of “play”. In contrast, the bottom',\n",
       " '“game” as nouns) but concentrated in the sports-\\nrelated senses of “play”. In contrast, the bottom\\ntwo rows show nearest neighbor sentences from\\nthe SemCor dataset (see below) using the biLM’s\\ncontext representation of “play” in the source sen-',\n",
       " 'the SemCor dataset (see below) using the biLM’s\\ncontext representation of “play” in the source sen-\\ntence. In these cases, the biLM is able to disam-\\nbiguate both the part of speech and word sense in\\nthe source sentence.',\n",
       " 'biguate both the part of speech and word sense in\\nthe source sentence.\\nThese observations can be quantiﬁed using anModel Acc.\\nCollobert et al. (2011) 97.3\\nMa and Hovy (2016) 97.6',\n",
       " 'Collobert et al. (2011) 97.3\\nMa and Hovy (2016) 97.6\\nLing et al. (2015) 97.8\\nCoVe, First Layer 93.3\\nCoVe, Second Layer 92.8',\n",
       " 'CoVe, First Layer 93.3\\nCoVe, Second Layer 92.8\\nbiLM, First Layer 97.3\\nbiLM, Second Layer 96.8\\nTable 6: Test set POS tagging accuracies for PTB. For',\n",
       " 'biLM, Second Layer 96.8\\nTable 6: Test set POS tagging accuracies for PTB. For\\nCoVe and the biLM, we report scores for both the ﬁrst\\nand second layer biLSTMs.\\nintrinsic evaluation of the contextual representa-',\n",
       " 'and second layer biLSTMs.\\nintrinsic evaluation of the contextual representa-\\ntions similar to Belinkov et al. (2017). To isolate\\nthe information encoded by the biLM, the repre-\\nsentations are used to directly make predictions for',\n",
       " 'the information encoded by the biLM, the repre-\\nsentations are used to directly make predictions for\\na ﬁne grained word sense disambiguation (WSD)\\ntask and a POS tagging task. Using this approach,\\nit is also possible to compare to CoVe, and across',\n",
       " 'task and a POS tagging task. Using this approach,\\nit is also possible to compare to CoVe, and across\\neach of the individual layers.\\nWord sense disambiguation Given a sentence,\\nwe can use the biLM representations to predict',\n",
       " 'Word sense disambiguation Given a sentence,\\nwe can use the biLM representations to predict\\nthe sense of a target word using a simple 1-\\nnearest neighbor approach, similar to Melamud\\net al. (2016). To do so, we ﬁrst use the biLM',\n",
       " 'nearest neighbor approach, similar to Melamud\\net al. (2016). To do so, we ﬁrst use the biLM\\nto compute representations for all words in Sem-\\nCor 3.0, our training corpus (Miller et al., 1994),\\nand then take the average representation for each',\n",
       " 'Cor 3.0, our training corpus (Miller et al., 1994),\\nand then take the average representation for each\\nsense. At test time, we again use the biLM to com-\\npute representations for a given target word and\\ntake the nearest neighbor sense from the training',\n",
       " 'pute representations for a given target word and\\ntake the nearest neighbor sense from the training\\nset, falling back to the ﬁrst sense from WordNet\\nfor lemmas not observed during training.\\nTable 5 compares WSD results using the eval-',\n",
       " 'for lemmas not observed during training.\\nTable 5 compares WSD results using the eval-\\nuation framework from Raganato et al. (2017b)\\nacross the same suite of four test sets in Raganato\\net al. (2017a). Overall, the biLM top layer rep-',\n",
       " 'across the same suite of four test sets in Raganato\\net al. (2017a). Overall, the biLM top layer rep-\\nresentations have F 1of 69.0 and are better at\\nWSD then the ﬁrst layer. This is competitive with\\na state-of-the-art WSD-speciﬁc supervised model',\n",
       " 'WSD then the ﬁrst layer. This is competitive with\\na state-of-the-art WSD-speciﬁc supervised model\\nusing hand crafted features (Iacobacci et al., 2016)\\nand a task speciﬁc biLSTM that is also trained\\nwith auxiliary coarse-grained semantic labels and',\n",
       " 'and a task speciﬁc biLSTM that is also trained\\nwith auxiliary coarse-grained semantic labels and\\nPOS tags (Raganato et al., 2017a). The CoVe\\nbiLSTM layers follow a similar pattern to those\\nfrom the biLM (higher overall performance at the',\n",
       " 'biLSTM layers follow a similar pattern to those\\nfrom the biLM (higher overall performance at the\\nsecond layer compared to the ﬁrst); however, our\\nbiLM outperforms the CoVe biLSTM, which trails\\nthe WordNet ﬁrst sense baseline.',\n",
       " 'biLM outperforms the CoVe biLSTM, which trails\\nthe WordNet ﬁrst sense baseline.\\nPOS tagging To examine whether the biLM\\ncaptures basic syntax, we used the context repre-\\nsentations as input to a linear classiﬁer that pre-',\n",
       " 'captures basic syntax, we used the context repre-\\nsentations as input to a linear classiﬁer that pre-\\ndicts POS tags with the Wall Street Journal portion\\nof the Penn Treebank (PTB) (Marcus et al., 1993).\\nAs the linear classiﬁer adds only a small amount',\n",
       " 'of the Penn Treebank (PTB) (Marcus et al., 1993).\\nAs the linear classiﬁer adds only a small amount\\nof model capacity, this is direct test of the biLM’s\\nrepresentations. Similar to WSD, the biLM rep-\\nresentations are competitive with carefully tuned,',\n",
       " 'representations. Similar to WSD, the biLM rep-\\nresentations are competitive with carefully tuned,\\ntask speciﬁc biLSTMs (Ling et al., 2015; Ma and\\nHovy, 2016). However, unlike WSD, accuracies\\nusing the ﬁrst biLM layer are higher than the',\n",
       " 'Hovy, 2016). However, unlike WSD, accuracies\\nusing the ﬁrst biLM layer are higher than the\\ntop layer, consistent with results from deep biL-\\nSTMs in multi-task training (Søgaard and Gold-\\nberg, 2016; Hashimoto et al., 2017) and MT (Be-',\n",
       " 'STMs in multi-task training (Søgaard and Gold-\\nberg, 2016; Hashimoto et al., 2017) and MT (Be-\\nlinkov et al., 2017). CoVe POS tagging accuracies\\nfollow the same pattern as those from the biLM,\\nand just like for WSD, the biLM achieves higher',\n",
       " 'follow the same pattern as those from the biLM,\\nand just like for WSD, the biLM achieves higher\\naccuracies than the CoVe encoder.\\nImplications for supervised tasks Taken to-\\ngether, these experiments conﬁrm different layers',\n",
       " 'Implications for supervised tasks Taken to-\\ngether, these experiments conﬁrm different layers\\nin the biLM represent different types of informa-\\ntion and explain why including all biLM layers is\\nimportant for the highest performance in down-',\n",
       " 'tion and explain why including all biLM layers is\\nimportant for the highest performance in down-\\nstream tasks. In addition, the biLM’s representa-\\ntions are more transferable to WSD and POS tag-\\nging than those in CoVe, helping to illustrate why',\n",
       " 'tions are more transferable to WSD and POS tag-\\nging than those in CoVe, helping to illustrate why\\nELMo outperforms CoVe in downstream tasks.\\n5.4 Sample efﬁciency\\nAdding ELMo to a model increases the sample ef-',\n",
       " '5.4 Sample efﬁciency\\nAdding ELMo to a model increases the sample ef-\\nﬁciency considerably, both in terms of number of\\nparameter updates to reach state-of-the-art perfor-\\nmance and the overall training set size. For ex-',\n",
       " 'parameter updates to reach state-of-the-art perfor-\\nmance and the overall training set size. For ex-\\nample, the SRL model reaches a maximum devel-\\nopment F 1after 486 epochs of training without\\nELMo. After adding ELMo, the model exceeds',\n",
       " 'opment F 1after 486 epochs of training without\\nELMo. After adding ELMo, the model exceeds\\nthe baseline maximum at epoch 10, a 98% relative\\ndecrease in the number of updates needed to reach\\nFigure 1: Comparison of baseline vs. ELMo perfor-',\n",
       " 'decrease in the number of updates needed to reach\\nFigure 1: Comparison of baseline vs. ELMo perfor-\\nmance for SNLI and SRL as the training set size is var-\\nied from 0.1% to 100%.\\nFigure 2: Visualization of softmax normalized biLM',\n",
       " 'ied from 0.1% to 100%.\\nFigure 2: Visualization of softmax normalized biLM\\nlayer weights across tasks and ELMo locations. Nor-\\nmalized weights less then 1=3are hatched with hori-\\nzontal lines and those greater then 2=3are speckled.',\n",
       " 'malized weights less then 1=3are hatched with hori-\\nzontal lines and those greater then 2=3are speckled.\\nthe same level of performance.\\nIn addition, ELMo-enhanced models use\\nsmaller training sets more efﬁciently than mod-',\n",
       " 'In addition, ELMo-enhanced models use\\nsmaller training sets more efﬁciently than mod-\\nels without ELMo. Figure 1 compares the per-\\nformance of baselines models with and without\\nELMo as the percentage of the full training set is',\n",
       " 'formance of baselines models with and without\\nELMo as the percentage of the full training set is\\nvaried from 0.1% to 100%. Improvements with\\nELMo are largest for smaller training sets and\\nsigniﬁcantly reduce the amount of training data',\n",
       " 'ELMo are largest for smaller training sets and\\nsigniﬁcantly reduce the amount of training data\\nneeded to reach a given level of performance. In\\nthe SRL case, the ELMo model with 1% of the\\ntraining set has about the same F 1as the baseline',\n",
       " 'the SRL case, the ELMo model with 1% of the\\ntraining set has about the same F 1as the baseline\\nmodel with 10% of the training set.\\n5.5 Visualization of learned weights\\nFigure 2 visualizes the softmax-normalized',\n",
       " '5.5 Visualization of learned weights\\nFigure 2 visualizes the softmax-normalized\\nlearned layer weights. At the input layer, the\\ntask model favors the ﬁrst biLSTM layer. For\\ncoreference and SQuAD, the this is strongly',\n",
       " 'task model favors the ﬁrst biLSTM layer. For\\ncoreference and SQuAD, the this is strongly\\nfavored, but the distribution is less peaked for\\nthe other tasks. The output layer weights are\\nrelatively balanced, with a slight preference for',\n",
       " 'the other tasks. The output layer weights are\\nrelatively balanced, with a slight preference for\\nthe lower layers.\\n6 Conclusion\\nWe have introduced a general approach for learn-',\n",
       " '6 Conclusion\\nWe have introduced a general approach for learn-\\ning high-quality deep context-dependent represen-\\ntations from biLMs, and shown large improve-\\nments when applying ELMo to a broad range of',\n",
       " 'tations from biLMs, and shown large improve-\\nments when applying ELMo to a broad range of\\nNLP tasks. Through ablations and other controlled\\nexperiments, we have also conﬁrmed that the\\nbiLM layers efﬁciently encode different types of',\n",
       " 'experiments, we have also conﬁrmed that the\\nbiLM layers efﬁciently encode different types of\\nsyntactic and semantic information about words-\\nin-context, and that using all layers improves over-\\nall task performance.',\n",
       " 'in-context, and that using all layers improves over-\\nall task performance.\\nReferences\\nJimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. 2016.\\nLayer normalization. CoRR abs/1607.06450.',\n",
       " 'Jimmy Ba, Ryan Kiros, and Geoffrey E. Hinton. 2016.\\nLayer normalization. CoRR abs/1607.06450.\\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-\\nsan Sajjad, and James R. Glass. 2017. What do neu-\\nral machine translation models learn about morphol-',\n",
       " 'san Sajjad, and James R. Glass. 2017. What do neu-\\nral machine translation models learn about morphol-\\nogy? In ACL.\\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\\nTomas Mikolov. 2017. Enriching word vectors with',\n",
       " 'Piotr Bojanowski, Edouard Grave, Armand Joulin, and\\nTomas Mikolov. 2017. Enriching word vectors with\\nsubword information. TACL 5:135–146.\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large an-',\n",
       " 'Samuel R. Bowman, Gabor Angeli, Christopher Potts,\\nand Christopher D. Manning. 2015. A large an-\\nnotated corpus for learning natural language infer-\\nence. In Proceedings of the 2015 Conference on\\nEmpirical Methods in Natural Language Processing',\n",
       " 'ence. In Proceedings of the 2015 Conference on\\nEmpirical Methods in Natural Language Processing\\n(EMNLP) . Association for Computational Linguis-\\ntics.\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,',\n",
       " 'tics.\\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\\nThorsten Brants, Phillipp Koehn, and Tony Robin-\\nson. 2014. One billion word benchmark for mea-\\nsuring progress in statistical language modeling. In',\n",
       " 'son. 2014. One billion word benchmark for mea-\\nsuring progress in statistical language modeling. In\\nINTERSPEECH .\\nQian Chen, Xiao-Dan Zhu, Zhen-Hua Ling, Si Wei,\\nHui Jiang, and Diana Inkpen. 2017. Enhanced lstm',\n",
       " 'Qian Chen, Xiao-Dan Zhu, Zhen-Hua Ling, Si Wei,\\nHui Jiang, and Diana Inkpen. 2017. Enhanced lstm\\nfor natural language inference. In ACL.\\nJason Chiu and Eric Nichols. 2016. Named entity\\nrecognition with bidirectional LSTM-CNNs. In',\n",
       " 'Jason Chiu and Eric Nichols. 2016. Named entity\\nrecognition with bidirectional LSTM-CNNs. In\\nTACL .\\nKyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-\\ndanau, and Yoshua Bengio. 2014. On the properties',\n",
       " 'Kyunghyun Cho, Bart van Merrienboer, Dzmitry Bah-\\ndanau, and Yoshua Bengio. 2014. On the properties\\nof neural machine translation: Encoder-decoder ap-\\nproaches. In SSST@EMNLP .\\nChristopher Clark and Matthew Gardner. 2017. Sim-',\n",
       " 'proaches. In SSST@EMNLP .\\nChristopher Clark and Matthew Gardner. 2017. Sim-\\nple and effective multi-paragraph reading compre-\\nhension. CoRR abs/1710.10723.\\nKevin Clark and Christopher D. Manning. 2016. Deep',\n",
       " 'hension. CoRR abs/1710.10723.\\nKevin Clark and Christopher D. Manning. 2016. Deep\\nreinforcement learning for mention-ranking corefer-\\nence models. In EMNLP .Ronan Collobert, Jason Weston, L ´eon Bottou, Michael\\nKarlen, Koray Kavukcuoglu, and Pavel P. Kuksa.',\n",
       " 'ence models. In EMNLP .Ronan Collobert, Jason Weston, L ´eon Bottou, Michael\\nKarlen, Koray Kavukcuoglu, and Pavel P. Kuksa.\\n2011. Natural language processing (almost) from\\nscratch. In JMLR .\\nAndrew M. Dai and Quoc V . Le. 2015. Semi-',\n",
       " 'scratch. In JMLR .\\nAndrew M. Dai and Quoc V . Le. 2015. Semi-\\nsupervised sequence learning. In NIPS .\\nGreg Durrett and Dan Klein. 2013. Easy victories and\\nuphill battles in coreference resolution. In EMNLP .',\n",
       " 'Greg Durrett and Dan Klein. 2013. Easy victories and\\nuphill battles in coreference resolution. In EMNLP .\\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\\nically grounded application of dropout in recurrent\\nneural networks. In NIPS .',\n",
       " 'ically grounded application of dropout in recurrent\\nneural networks. In NIPS .\\nYichen Gong, Heng Luo, and Jian Zhang. 2018. Nat-\\nural language inference over interaction space. In\\nICLR .',\n",
       " 'ural language inference over interaction space. In\\nICLR .\\nKazuma Hashimoto, Caiming Xiong, Yoshimasa Tsu-\\nruoka, and Richard Socher. 2017. A joint many-task\\nmodel: Growing a neural network for multiple nlp',\n",
       " 'ruoka, and Richard Socher. 2017. A joint many-task\\nmodel: Growing a neural network for multiple nlp\\ntasks. In EMNLP 2017 .\\nLuheng He, Kenton Lee, Mike Lewis, and Luke S.\\nZettlemoyer. 2017. Deep semantic role labeling:',\n",
       " 'Luheng He, Kenton Lee, Mike Lewis, and Luke S.\\nZettlemoyer. 2017. Deep semantic role labeling:\\nWhat works and what’s next. In ACL.\\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997. Long\\nshort-term memory. Neural Computation 9.',\n",
       " 'Sepp Hochreiter and J ¨urgen Schmidhuber. 1997. Long\\nshort-term memory. Neural Computation 9.\\nIgnacio Iacobacci, Mohammad Taher Pilehvar, and\\nRoberto Navigli. 2016. Embeddings for word sense\\ndisambiguation: An evaluation study. In ACL.',\n",
       " 'Roberto Navigli. 2016. Embeddings for word sense\\ndisambiguation: An evaluation study. In ACL.\\nRafal J ´ozefowicz, Oriol Vinyals, Mike Schuster, Noam\\nShazeer, and Yonghui Wu. 2016. Exploring the lim-\\nits of language modeling. CoRR abs/1602.02410.',\n",
       " 'Shazeer, and Yonghui Wu. 2016. Exploring the lim-\\nits of language modeling. CoRR abs/1602.02410.\\nRafal J ´ozefowicz, Wojciech Zaremba, and Ilya\\nSutskever. 2015. An empirical exploration of recur-\\nrent network architectures. In ICML .',\n",
       " 'Sutskever. 2015. An empirical exploration of recur-\\nrent network architectures. In ICML .\\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\\nder M Rush. 2015. Character-aware neural language\\nmodels. In AAAI 2016 .',\n",
       " 'der M Rush. 2015. Character-aware neural language\\nmodels. In AAAI 2016 .\\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\\nmethod for stochastic optimization. In ICLR .\\nAnkit Kumar, Ozan Irsoy, Peter Ondruska, Mohit',\n",
       " 'method for stochastic optimization. In ICLR .\\nAnkit Kumar, Ozan Irsoy, Peter Ondruska, Mohit\\nIyyer, Ishaan Gulrajani James Bradbury, Victor\\nZhong, Romain Paulus, and Richard Socher. 2016.\\nAsk me anything: Dynamic memory networks for',\n",
       " 'Zhong, Romain Paulus, and Richard Socher. 2016.\\nAsk me anything: Dynamic memory networks for\\nnatural language processing. In ICML .\\nJohn D. Lafferty, Andrew McCallum, and Fernando\\nPereira. 2001. Conditional random ﬁelds: Prob-',\n",
       " 'John D. Lafferty, Andrew McCallum, and Fernando\\nPereira. 2001. Conditional random ﬁelds: Prob-\\nabilistic models for segmenting and labeling se-\\nquence data. In ICML .\\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-',\n",
       " 'quence data. In ICML .\\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\\nNeural architectures for named entity recognition.\\nInNAACL-HLT .',\n",
       " 'Neural architectures for named entity recognition.\\nInNAACL-HLT .\\nKenton Lee, Luheng He, Mike Lewis, and Luke S.\\nZettlemoyer. 2017. End-to-end neural coreference\\nresolution. In EMNLP .',\n",
       " 'Zettlemoyer. 2017. End-to-end neural coreference\\nresolution. In EMNLP .\\nWang Ling, Chris Dyer, Alan W. Black, Isabel Tran-\\ncoso, Ramon Fermandez, Silvio Amir, Lu ´ıs Marujo,\\nand Tiago Lu ´ıs. 2015. Finding function in form:',\n",
       " 'coso, Ramon Fermandez, Silvio Amir, Lu ´ıs Marujo,\\nand Tiago Lu ´ıs. 2015. Finding function in form:\\nCompositional character models for open vocabu-\\nlary word representation. In EMNLP .\\nXiaodong Liu, Yelong Shen, Kevin Duh, and Jian-',\n",
       " 'lary word representation. In EMNLP .\\nXiaodong Liu, Yelong Shen, Kevin Duh, and Jian-\\nfeng Gao. 2017. Stochastic answer networks for\\nmachine reading comprehension. arXiv preprint\\narXiv:1712.03556 .',\n",
       " 'machine reading comprehension. arXiv preprint\\narXiv:1712.03556 .\\nXuezhe Ma and Eduard H. Hovy. 2016. End-to-end\\nsequence labeling via bi-directional LSTM-CNNs-\\nCRF. In ACL.',\n",
       " 'sequence labeling via bi-directional LSTM-CNNs-\\nCRF. In ACL.\\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\\nMarcinkiewicz. 1993. Building a large annotated\\ncorpus of english: The penn treebank. Computa-',\n",
       " 'Marcinkiewicz. 1993. Building a large annotated\\ncorpus of english: The penn treebank. Computa-\\ntional Linguistics 19:313–330.\\nBryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-',\n",
       " 'Bryan McCann, James Bradbury, Caiming Xiong, and\\nRichard Socher. 2017. Learned in translation: Con-\\ntextualized word vectors. In NIPS 2017 .\\nOren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-',\n",
       " 'Oren Melamud, Jacob Goldberger, and Ido Dagan.\\n2016. context2vec: Learning generic context em-\\nbedding with bidirectional lstm. In CoNLL .\\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2017. On\\nthe state of the art of evaluation in neural language',\n",
       " 'G´abor Melis, Chris Dyer, and Phil Blunsom. 2017. On\\nthe state of the art of evaluation in neural language\\nmodels. CoRR abs/1707.05589.\\nStephen Merity, Nitish Shirish Keskar, and Richard\\nSocher. 2017. Regularizing and optimizing lstm lan-',\n",
       " 'Stephen Merity, Nitish Shirish Keskar, and Richard\\nSocher. 2017. Regularizing and optimizing lstm lan-\\nguage models. CoRR abs/1708.02182.\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-',\n",
       " 'Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\\nrado, and Jeff Dean. 2013. Distributed representa-\\ntions of words and phrases and their compositional-\\nity. In NIPS .\\nGeorge A. Miller, Martin Chodorow, Shari Landes,',\n",
       " 'ity. In NIPS .\\nGeorge A. Miller, Martin Chodorow, Shari Landes,\\nClaudia Leacock, and Robert G. Thomas. 1994. Us-\\ning a semantic concordance for sense identiﬁcation.\\nInHLT .',\n",
       " 'ing a semantic concordance for sense identiﬁcation.\\nInHLT .\\nTsendsuren Munkhdalai and Hong Yu. 2017. Neural\\ntree indexers for text understanding. In EACL .\\nArvind Neelakantan, Jeevan Shankar, Alexandre Pas-',\n",
       " 'tree indexers for text understanding. In EACL .\\nArvind Neelakantan, Jeevan Shankar, Alexandre Pas-\\nsos, and Andrew McCallum. 2014. Efﬁcient non-\\nparametric estimation of multiple embeddings per\\nword in vector space. In EMNLP .',\n",
       " 'parametric estimation of multiple embeddings per\\nword in vector space. In EMNLP .\\nMartha Palmer, Paul Kingsbury, and Daniel Gildea.\\n2005. The proposition bank: An annotated corpus of\\nsemantic roles. Computational Linguistics 31:71–',\n",
       " '2005. The proposition bank: An annotated corpus of\\nsemantic roles. Computational Linguistics 31:71–\\n106.\\nJeffrey Pennington, Richard Socher, and Christo-\\npher D. Manning. 2014. Glove: Global vectors for',\n",
       " 'Jeffrey Pennington, Richard Socher, and Christo-\\npher D. Manning. 2014. Glove: Global vectors for\\nword representation. In EMNLP .Matthew E. Peters, Waleed Ammar, Chandra Bhaga-\\nvatula, and Russell Power. 2017. Semi-supervised\\nsequence tagging with bidirectional language mod-',\n",
       " 'vatula, and Russell Power. 2017. Semi-supervised\\nsequence tagging with bidirectional language mod-\\nels. In ACL.\\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\\nHwee Tou Ng, Anders Bj ¨orkelund, Olga Uryupina,',\n",
       " 'Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,\\nHwee Tou Ng, Anders Bj ¨orkelund, Olga Uryupina,\\nYuchen Zhang, and Zhi Zhong. 2013. Towards ro-\\nbust linguistic analysis using ontonotes. In CoNLL .\\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,',\n",
       " 'bust linguistic analysis using ontonotes. In CoNLL .\\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\\nOlga Uryupina, and Yuchen Zhang. 2012. Conll-\\n2012 shared task: Modeling multilingual unre-\\nstricted coreference in ontonotes. In EMNLP-',\n",
       " '2012 shared task: Modeling multilingual unre-\\nstricted coreference in ontonotes. In EMNLP-\\nCoNLL Shared Task .\\nAlessandro Raganato, Claudio Delli Bovi, and Roberto\\nNavigli. 2017a. Neural sequence learning models',\n",
       " 'Alessandro Raganato, Claudio Delli Bovi, and Roberto\\nNavigli. 2017a. Neural sequence learning models\\nfor word sense disambiguation. In EMNLP .\\nAlessandro Raganato, Jose Camacho-Collados, and\\nRoberto Navigli. 2017b. Word sense disambigua-',\n",
       " 'Alessandro Raganato, Jose Camacho-Collados, and\\nRoberto Navigli. 2017b. Word sense disambigua-\\ntion: A uniﬁed evaluation framework and empirical\\ncomparison. In EACL .\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and',\n",
       " 'comparison. In EACL .\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\\nPercy Liang. 2016. Squad: 100, 000+ questions for\\nmachine comprehension of text. In EMNLP .\\nPrajit Ramachandran, Peter Liu, and Quoc Le. 2017.',\n",
       " 'machine comprehension of text. In EMNLP .\\nPrajit Ramachandran, Peter Liu, and Quoc Le. 2017.\\nImproving sequence to sequence learning with unla-\\nbeled data. In EMNLP .\\nErik F. Tjong Kim Sang and Fien De Meulder.',\n",
       " 'beled data. In EMNLP .\\nErik F. Tjong Kim Sang and Fien De Meulder.\\n2003. Introduction to the CoNLL-2003 shared task:\\nLanguage-independent named entity recognition. In\\nCoNLL .',\n",
       " 'Language-independent named entity recognition. In\\nCoNLL .\\nMin Joon Seo, Aniruddha Kembhavi, Ali Farhadi, and\\nHannaneh Hajishirzi. 2017. Bidirectional attention\\nﬂow for machine comprehension. In ICLR .',\n",
       " 'Hannaneh Hajishirzi. 2017. Bidirectional attention\\nﬂow for machine comprehension. In ICLR .\\nRichard Socher, Alex Perelygin, Jean Y Wu, Jason\\nChuang, Christopher D Manning, Andrew Y Ng,\\nand Christopher Potts. 2013. Recursive deep mod-',\n",
       " 'Chuang, Christopher D Manning, Andrew Y Ng,\\nand Christopher Potts. 2013. Recursive deep mod-\\nels for semantic compositionality over a sentiment\\ntreebank. In EMNLP .\\nAnders Søgaard and Yoav Goldberg. 2016. Deep',\n",
       " 'treebank. In EMNLP .\\nAnders Søgaard and Yoav Goldberg. 2016. Deep\\nmulti-task learning with low level tasks supervised\\nat lower layers. In ACL 2016 .\\nNitish Srivastava, Geoffrey E. Hinton, Alex',\n",
       " 'at lower layers. In ACL 2016 .\\nNitish Srivastava, Geoffrey E. Hinton, Alex\\nKrizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. 2014. Dropout: a simple way to prevent neural\\nnetworks from overﬁtting. Journal of Machine',\n",
       " 'nov. 2014. Dropout: a simple way to prevent neural\\nnetworks from overﬁtting. Journal of Machine\\nLearning Research 15:1929–1958.\\nRupesh Kumar Srivastava, Klaus Greff, and J ¨urgen\\nSchmidhuber. 2015. Training very deep networks.',\n",
       " 'Rupesh Kumar Srivastava, Klaus Greff, and J ¨urgen\\nSchmidhuber. 2015. Training very deep networks.\\nInNIPS .\\nJoseph P. Turian, Lev-Arie Ratinov, and Yoshua Ben-\\ngio. 2010. Word representations: A simple and gen-',\n",
       " 'Joseph P. Turian, Lev-Arie Ratinov, and Yoshua Ben-\\ngio. 2010. Word representations: A simple and gen-\\neral method for semi-supervised learning. In ACL.\\nWenhui Wang, Nan Yang, Furu Wei, Baobao Chang,\\nand Ming Zhou. 2017. Gated self-matching net-',\n",
       " 'Wenhui Wang, Nan Yang, Furu Wei, Baobao Chang,\\nand Ming Zhou. 2017. Gated self-matching net-\\nworks for reading comprehension and question an-\\nswering. In ACL.\\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen',\n",
       " 'swering. In ACL.\\nJohn Wieting, Mohit Bansal, Kevin Gimpel, and Karen\\nLivescu. 2016. Charagram: Embedding words and\\nsentences via character n-grams. In EMNLP .\\nSam Wiseman, Alexander M. Rush, and Stuart M.',\n",
       " 'sentences via character n-grams. In EMNLP .\\nSam Wiseman, Alexander M. Rush, and Stuart M.\\nShieber. 2016. Learning global features for coref-\\nerence resolution. In HLT-NAACL .\\nMatthew D. Zeiler. 2012. Adadelta: An adaptive learn-',\n",
       " 'erence resolution. In HLT-NAACL .\\nMatthew D. Zeiler. 2012. Adadelta: An adaptive learn-\\ning rate method. CoRR abs/1212.5701.\\nJie Zhou and Wei Xu. 2015. End-to-end learning of\\nsemantic role labeling using recurrent neural net-',\n",
       " 'Jie Zhou and Wei Xu. 2015. End-to-end learning of\\nsemantic role labeling using recurrent neural net-\\nworks. In ACL.\\nPeng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu,\\nHongyun Bao, and Bo Xu. 2016. Text classiﬁcation',\n",
       " 'Peng Zhou, Zhenyu Qi, Suncong Zheng, Jiaming Xu,\\nHongyun Bao, and Bo Xu. 2016. Text classiﬁcation\\nimproved by integrating bidirectional lstm with two-\\ndimensional max pooling. In COLING .\\nA Supplemental Material to accompany',\n",
       " 'dimensional max pooling. In COLING .\\nA Supplemental Material to accompany\\nDeep contextualized word\\nrepresentations\\nThis supplement contains details of the model ar-',\n",
       " 'representations\\nThis supplement contains details of the model ar-\\nchitectures, training routines and hyper-parameter\\nchoices for the state-of-the-art models in Section\\n4.',\n",
       " 'choices for the state-of-the-art models in Section\\n4.\\nAll of the individual models share a common ar-\\nchitecture in the lowest layers with a context inde-\\npendent token representation below several layers',\n",
       " 'chitecture in the lowest layers with a context inde-\\npendent token representation below several layers\\nof stacked RNNs – LSTMs in every case except\\nthe SQuAD model that uses GRUs.\\nA.1 Fine tuning biLM',\n",
       " 'the SQuAD model that uses GRUs.\\nA.1 Fine tuning biLM\\nAs noted in Sec. 3.4, ﬁne tuning the biLM on task\\nspeciﬁc data typically resulted in signiﬁcant drops\\nin perplexity. To ﬁne tune on a given task, the',\n",
       " 'speciﬁc data typically resulted in signiﬁcant drops\\nin perplexity. To ﬁne tune on a given task, the\\nsupervised labels were temporarily ignored, the\\nbiLM ﬁne tuned for one epoch on the training split\\nand evaluated on the development split. Once ﬁne',\n",
       " 'biLM ﬁne tuned for one epoch on the training split\\nand evaluated on the development split. Once ﬁne\\ntuned, the biLM weights were ﬁxed during task\\ntraining.\\nTable 7 lists the development set perplexities for',\n",
       " 'training.\\nTable 7 lists the development set perplexities for\\nthe considered tasks. In every case except CoNLL\\n2012, ﬁne tuning results in a large improvement in\\nperplexity, e.g., from 72.1 to 16.8 for SNLI.',\n",
       " '2012, ﬁne tuning results in a large improvement in\\nperplexity, e.g., from 72.1 to 16.8 for SNLI.\\nThe impact of ﬁne tuning on supervised perfor-\\nmance is task dependent. In the case of SNLI,\\nﬁne tuning the biLM increased development accu-',\n",
       " 'mance is task dependent. In the case of SNLI,\\nﬁne tuning the biLM increased development accu-\\nracy 0.6% from 88.9% to 89.5% for our single best\\nmodel. However, for sentiment classiﬁcation de-\\nvelopment set accuracy is approximately the same',\n",
       " 'model. However, for sentiment classiﬁcation de-\\nvelopment set accuracy is approximately the same\\nregardless whether a ﬁne tuned biLM was used.\\nA.2 Importance of \\rin Eqn. (1)\\nThe\\rparameter in Eqn. (1) was of practical im-',\n",
       " 'A.2 Importance of \\rin Eqn. (1)\\nThe\\rparameter in Eqn. (1) was of practical im-\\nportance to aid optimization, due to the differ-\\nent distributions between the biLM internal rep-\\nresentations and the task speciﬁc representations.',\n",
       " 'ent distributions between the biLM internal rep-\\nresentations and the task speciﬁc representations.\\nIt is especially important in the last-only case in\\nSec. 5.1. Without this parameter, the last-only\\ncase performed poorly (well below the baseline)',\n",
       " 'Sec. 5.1. Without this parameter, the last-only\\ncase performed poorly (well below the baseline)\\nfor SNLI and training failed completely for SRL.\\nA.3 Textual Entailment\\nOur baseline SNLI model is the ESIM sequence',\n",
       " 'A.3 Textual Entailment\\nOur baseline SNLI model is the ESIM sequence\\nmodel from Chen et al. (2017). Following the\\noriginal implementation, we used 300 dimensions\\nfor all LSTM and feed forward layers and pre-',\n",
       " 'original implementation, we used 300 dimensions\\nfor all LSTM and feed forward layers and pre-\\ntrained 300 dimensional GloVe embeddings that\\nwere ﬁxed during training. For regularization, weDatasetBefore\\ntuningAfter',\n",
       " 'were ﬁxed during training. For regularization, weDatasetBefore\\ntuningAfter\\ntuning\\nSNLI 72.1 16.8\\nCoNLL 2012 (coref/SRL) 92.3 -',\n",
       " 'SNLI 72.1 16.8\\nCoNLL 2012 (coref/SRL) 92.3 -\\nCoNLL 2003 (NER) 103.2 46.3\\nSQuADContext 99.1 43.5\\nQuestions 158.2 52.0',\n",
       " 'SQuADContext 99.1 43.5\\nQuestions 158.2 52.0\\nSST 131.5 78.6\\nTable 7: Development set perplexity before and after\\nﬁne tuning for one epoch on the training set for vari-',\n",
       " 'Table 7: Development set perplexity before and after\\nﬁne tuning for one epoch on the training set for vari-\\nous datasets (lower is better). Reported values are the\\naverage of the forward and backward perplexities.\\nadded 50% variational dropout (Gal and Ghahra-',\n",
       " 'average of the forward and backward perplexities.\\nadded 50% variational dropout (Gal and Ghahra-\\nmani, 2016) to the input of each LSTM layer and\\n50% dropout (Srivastava et al., 2014) at the input\\nto the ﬁnal two fully connected layers. All feed',\n",
       " '50% dropout (Srivastava et al., 2014) at the input\\nto the ﬁnal two fully connected layers. All feed\\nforward layers use ReLU activations. Parame-\\nters were optimized using Adam (Kingma and Ba,\\n2015) with gradient norms clipped at 5.0 and ini-',\n",
       " 'ters were optimized using Adam (Kingma and Ba,\\n2015) with gradient norms clipped at 5.0 and ini-\\ntial learning rate 0.0004, decreasing by half each\\ntime accuracy on the development set did not in-\\ncrease in subsequent epochs. The batch size was',\n",
       " 'time accuracy on the development set did not in-\\ncrease in subsequent epochs. The batch size was\\n32.\\nThe best ELMo conﬁguration added ELMo vec-\\ntors to both the input and output of the lowest',\n",
       " 'The best ELMo conﬁguration added ELMo vec-\\ntors to both the input and output of the lowest\\nlayer LSTM, using (1) with layer normalization\\nand\\x15= 0:001. Due to the increased number of\\nparameters in the ELMo model, we added `2reg-',\n",
       " 'and\\x15= 0:001. Due to the increased number of\\nparameters in the ELMo model, we added `2reg-\\nularization with regularization coefﬁcient 0.0001\\nto all recurrent and feed forward weight matrices\\nand 50% dropout after the attention layer.',\n",
       " 'to all recurrent and feed forward weight matrices\\nand 50% dropout after the attention layer.\\nTable 8 compares test set accuracy of our sys-\\ntem to previously published systems. Overall,\\nadding ELMo to the ESIM model improved ac-',\n",
       " 'tem to previously published systems. Overall,\\nadding ELMo to the ESIM model improved ac-\\ncuracy by 0.7% establishing a new single model\\nstate-of-the-art of 88.7%, and a ﬁve member en-\\nsemble pushes the overall accuracy to 89.3%.',\n",
       " 'state-of-the-art of 88.7%, and a ﬁve member en-\\nsemble pushes the overall accuracy to 89.3%.\\nA.4 Question Answering\\nOur QA model is a simpliﬁed version of the model\\nfrom Clark and Gardner (2017). It embeds to-',\n",
       " 'Our QA model is a simpliﬁed version of the model\\nfrom Clark and Gardner (2017). It embeds to-\\nkens by concatenating each token’s case-sensitive\\n300 dimensional GloVe word vector (Penning-\\nton et al., 2014) with a character-derived embed-',\n",
       " '300 dimensional GloVe word vector (Penning-\\nton et al., 2014) with a character-derived embed-\\nding produced using a convolutional neural net-\\nwork followed by max-pooling on learned char-\\nacter embeddings. The token embeddings are',\n",
       " 'work followed by max-pooling on learned char-\\nacter embeddings. The token embeddings are\\npassed through a shared bi-directional GRU, and\\nthen the bi-directional attention mechanism from\\nBiDAF (Seo et al., 2017). The augmented con-',\n",
       " 'then the bi-directional attention mechanism from\\nBiDAF (Seo et al., 2017). The augmented con-\\nModel Acc.\\nFeature based (Bowman et al., 2015) 78.2\\nDIIN (Gong et al., 2018) 88.0',\n",
       " 'Feature based (Bowman et al., 2015) 78.2\\nDIIN (Gong et al., 2018) 88.0\\nBCN+Char+CoVe (McCann et al., 2017) 88.1\\nESIM (Chen et al., 2017) 88.0\\nESIM+TreeLSTM (Chen et al., 2017) 88.6',\n",
       " 'ESIM (Chen et al., 2017) 88.0\\nESIM+TreeLSTM (Chen et al., 2017) 88.6\\nESIM+ELMo 88.7\\x060.17\\nDIIN ensemble (Gong et al., 2018) 88.9\\nESIM+ELMo ensemble 89.3',\n",
       " 'DIIN ensemble (Gong et al., 2018) 88.9\\nESIM+ELMo ensemble 89.3\\nTable 8: SNLI test set accuracy.3Single model results occupy the portion, with ensemble results at the bottom.\\ntext vectors are then passed through a linear layer\\nwith ReLU activations, a residual self-attention',\n",
       " 'text vectors are then passed through a linear layer\\nwith ReLU activations, a residual self-attention\\nlayer that uses a GRU followed by the same atten-\\ntion mechanism applied context-to-context, and\\nanother linear layer with ReLU activations. Fi-',\n",
       " 'tion mechanism applied context-to-context, and\\nanother linear layer with ReLU activations. Fi-\\nnally, the results are fed through linear layers to\\npredict the start and end token of the answer.\\nVariational dropout is used before the input to',\n",
       " 'predict the start and end token of the answer.\\nVariational dropout is used before the input to\\nthe GRUs and the linear layers at a rate of 0.2. A\\ndimensionality of 90 is used for the GRUs, and\\n180 for the linear layers. We optimize the model',\n",
       " 'dimensionality of 90 is used for the GRUs, and\\n180 for the linear layers. We optimize the model\\nusing Adadelta with a batch size of 45. At test\\ntime we use an exponential moving average of the\\nweights and limit the output span to be of at most',\n",
       " 'time we use an exponential moving average of the\\nweights and limit the output span to be of at most\\nsize 17. We do not update the word vectors during\\ntraining.\\nPerformance was highest when adding ELMo',\n",
       " 'training.\\nPerformance was highest when adding ELMo\\nwithout layer normalization to both the input and\\noutput of the contextual GRU layer and leaving the\\nELMo weights unregularized ( \\x15= 0).',\n",
       " 'output of the contextual GRU layer and leaving the\\nELMo weights unregularized ( \\x15= 0).\\nTable 9 compares test set results from the\\nSQuAD leaderboard as of November 17, 2017\\nwhen we submitted our system. Overall, our sub-',\n",
       " 'SQuAD leaderboard as of November 17, 2017\\nwhen we submitted our system. Overall, our sub-\\nmission had the highest single model and ensem-\\nble results, improving the previous single model\\nresult (SAN) by 1.4% F 1and our baseline by',\n",
       " 'ble results, improving the previous single model\\nresult (SAN) by 1.4% F 1and our baseline by\\n4.2%. A 11 member ensemble pushes F 1to\\n87.4%, 1.0% increase over the previous ensemble\\nbest.',\n",
       " '87.4%, 1.0% increase over the previous ensemble\\nbest.\\nA.5 Semantic Role Labeling\\nOur baseline SRL model is an exact reimplemen-\\ntation of (He et al., 2017). Words are represented',\n",
       " 'Our baseline SRL model is an exact reimplemen-\\ntation of (He et al., 2017). Words are represented\\nusing a concatenation of 100 dimensional vector\\nrepresentations, initialized using GloVe (Penning-\\nton et al., 2014) and a binary, per-word predicate',\n",
       " 'representations, initialized using GloVe (Penning-\\nton et al., 2014) and a binary, per-word predicate\\nfeature, represented using an 100 dimensional em-\\n3A comprehensive comparison can be found at https:\\n//nlp.stanford.edu/projects/snli/bedding. This 200 dimensional token represen-',\n",
       " '3A comprehensive comparison can be found at https:\\n//nlp.stanford.edu/projects/snli/bedding. This 200 dimensional token represen-\\ntation is then passed through an 8 layer “inter-\\nleaved” biLSTM with a 300 dimensional hidden\\nsize, in which the directions of the LSTM layers',\n",
       " 'leaved” biLSTM with a 300 dimensional hidden\\nsize, in which the directions of the LSTM layers\\nalternate per layer. This deep LSTM uses High-\\nway connections (Srivastava et al., 2015) between\\nlayers and variational recurrent dropout (Gal and',\n",
       " 'way connections (Srivastava et al., 2015) between\\nlayers and variational recurrent dropout (Gal and\\nGhahramani, 2016). This deep representation is\\nthen projected using a ﬁnal dense layer followed\\nby a softmax activation to form a distribution over',\n",
       " 'then projected using a ﬁnal dense layer followed\\nby a softmax activation to form a distribution over\\nall possible tags. Labels consist of semantic roles\\nfrom PropBank (Palmer et al., 2005) augmented\\nwith a BIO labeling scheme to represent argu-',\n",
       " 'from PropBank (Palmer et al., 2005) augmented\\nwith a BIO labeling scheme to represent argu-\\nment spans. During training, we minimize the\\nnegative log likelihood of the tag sequence using\\nAdadelta with a learning rate of 1.0 and \\x1a= 0:95',\n",
       " 'negative log likelihood of the tag sequence using\\nAdadelta with a learning rate of 1.0 and \\x1a= 0:95\\n(Zeiler, 2012). At test time, we perform Viterbi\\ndecoding to enforce valid spans using BIO con-\\nstraints. Variational dropout of 10% is added to',\n",
       " 'decoding to enforce valid spans using BIO con-\\nstraints. Variational dropout of 10% is added to\\nall LSTM hidden layers. Gradients are clipped if\\ntheir value exceeds 1.0. Models are trained for 500\\nepochs or until validation F1 does not improve for',\n",
       " 'their value exceeds 1.0. Models are trained for 500\\nepochs or until validation F1 does not improve for\\n200 epochs, whichever is sooner. The pretrained\\nGloVe vectors are ﬁne-tuned during training. The\\nﬁnal dense layer and all cells of all LSTMs are ini-',\n",
       " 'GloVe vectors are ﬁne-tuned during training. The\\nﬁnal dense layer and all cells of all LSTMs are ini-\\ntialized to be orthogonal. The forget gate bias is\\ninitialized to 1 for all LSTMs, with all other gates\\ninitialized to 0, as per (J ´ozefowicz et al., 2015).',\n",
       " 'initialized to 1 for all LSTMs, with all other gates\\ninitialized to 0, as per (J ´ozefowicz et al., 2015).\\nTable 10 compares test set F1 scores of our\\nELMo augmented implementation of (He et al.,\\n2017) with previous results. Our single model',\n",
       " 'ELMo augmented implementation of (He et al.,\\n2017) with previous results. Our single model\\nscore of 84.6 F1 represents a new state-of-the-art\\nresult on the CONLL 2012 Semantic Role Label-\\ning task, surpassing the previous single model re-',\n",
       " 'result on the CONLL 2012 Semantic Role Label-\\ning task, surpassing the previous single model re-\\nsult by 2.9 F1 and a 5-model ensemble by 1.2 F1.\\nA.6 Coreference resolution\\nOur baseline coreference model is the end-to-end',\n",
       " 'A.6 Coreference resolution\\nOur baseline coreference model is the end-to-end\\nneural model from Lee et al. (2017) with all hy-\\nModel EM F1\\nBiDAF (Seo et al., 2017) 68.0 77.3',\n",
       " 'Model EM F1\\nBiDAF (Seo et al., 2017) 68.0 77.3\\nBiDAF + Self Attention 72.1 81.1\\nDCN+ 75.1 83.1\\nReg-RaSoR 75.8 83.3',\n",
       " 'DCN+ 75.1 83.1\\nReg-RaSoR 75.8 83.3\\nFusionNet 76.0 83.9\\nr-net (Wang et al., 2017) 76.5 84.3\\nSAN (Liu et al., 2017) 76.8 84.4',\n",
       " 'r-net (Wang et al., 2017) 76.5 84.3\\nSAN (Liu et al., 2017) 76.8 84.4\\nBiDAF + Self Attention + ELMo 78.6 85.8\\nDCN+ Ensemble 78.9 86.0\\nFusionNet Ensemble 79.0 86.0',\n",
       " 'DCN+ Ensemble 78.9 86.0\\nFusionNet Ensemble 79.0 86.0\\nInteractive AoA Reader+ Ensemble 79.1 86.5\\nBiDAF + Self Attention + ELMo Ensemble 81.0 87.4\\nTable 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1. The top half of the table contains',\n",
       " 'BiDAF + Self Attention + ELMo Ensemble 81.0 87.4\\nTable 9: Test set results for SQuAD, showing both Exact Match (EM) and F 1. The top half of the table contains\\nsingle model results with ensembles at the bottom. References provided where available.\\nModel F1\\nPradhan et al. (2013) 77.5',\n",
       " 'Model F1\\nPradhan et al. (2013) 77.5\\nZhou and Xu (2015) 81.3\\nHe et al. (2017), single 81.7\\nHe et al. (2017), ensemble 83.4',\n",
       " 'He et al. (2017), single 81.7\\nHe et al. (2017), ensemble 83.4\\nHe et al. (2017), our impl. 81.4\\nHe et al. (2017) + ELMo 84.6\\nTable 10: SRL CoNLL 2012 test set F 1.',\n",
       " 'He et al. (2017) + ELMo 84.6\\nTable 10: SRL CoNLL 2012 test set F 1.\\nModel Average F 1\\nDurrett and Klein (2013) 60.3\\nWiseman et al. (2016) 64.2',\n",
       " 'Durrett and Klein (2013) 60.3\\nWiseman et al. (2016) 64.2\\nClark and Manning (2016) 65.7\\nLee et al. (2017) (single) 67.2\\nLee et al. (2017) (ensemble) 68.8',\n",
       " 'Lee et al. (2017) (single) 67.2\\nLee et al. (2017) (ensemble) 68.8\\nLee et al. (2017) + ELMo 70.4\\nTable 11: Coreference resolution average F 1on the test\\nset from the CoNLL 2012 shared task.',\n",
       " 'Table 11: Coreference resolution average F 1on the test\\nset from the CoNLL 2012 shared task.\\nperparameters exactly following the original im-\\nplementation.\\nThe best conﬁguration added ELMo to the in-',\n",
       " 'plementation.\\nThe best conﬁguration added ELMo to the in-\\nput of the lowest layer biLSTM and weighted the\\nbiLM layers using (1) without any regularization\\n(\\x15= 0) or layer normalization. 50% dropout was',\n",
       " 'biLM layers using (1) without any regularization\\n(\\x15= 0) or layer normalization. 50% dropout was\\nadded to the ELMo representations.\\nTable 11 compares our results with previously\\npublished results. Overall, we improve the single',\n",
       " 'Table 11 compares our results with previously\\npublished results. Overall, we improve the single\\nmodel state-of-the-art by 3.2% average F 1, and our\\nsingle model result improves the previous ensem-\\nble best by 1.6% F 1. Adding ELMo to the output',\n",
       " 'single model result improves the previous ensem-\\nble best by 1.6% F 1. Adding ELMo to the output\\nfrom the biLSTM in addition to the biLSTM input\\nreduced F 1by approximately 0.7% (not shown).A.7 Named Entity Recognition\\nOur baseline NER model concatenates 50 dimen-',\n",
       " 'reduced F 1by approximately 0.7% (not shown).A.7 Named Entity Recognition\\nOur baseline NER model concatenates 50 dimen-\\nsional pre-trained Senna vectors (Collobert et al.,\\n2011) with a CNN character based representation.\\nThe character representation uses 16 dimensional',\n",
       " '2011) with a CNN character based representation.\\nThe character representation uses 16 dimensional\\ncharacter embeddings and 128 convolutional ﬁl-\\nters of width three characters, a ReLU activation\\nand by max pooling. The token representation is',\n",
       " 'ters of width three characters, a ReLU activation\\nand by max pooling. The token representation is\\npassed through two biLSTM layers, the ﬁrst with\\n200 hidden units and the second with 100 hid-\\nden units before a ﬁnal dense layer and softmax',\n",
       " '200 hidden units and the second with 100 hid-\\nden units before a ﬁnal dense layer and softmax\\nlayer. During training, we use a CRF loss and at\\ntest time perform decoding using the Viterbi algo-\\nrithm while ensuring that the output tag sequence',\n",
       " 'test time perform decoding using the Viterbi algo-\\nrithm while ensuring that the output tag sequence\\nis valid.\\nVariational dropout is added to the input of both\\nbiLSTM layers. During training the gradients are',\n",
       " 'Variational dropout is added to the input of both\\nbiLSTM layers. During training the gradients are\\nrescaled if their `2norm exceeds 5.0 and param-\\neters updated using Adam with constant learning\\nrate of 0.001. The pre-trained Senna embeddings',\n",
       " 'eters updated using Adam with constant learning\\nrate of 0.001. The pre-trained Senna embeddings\\nare ﬁne tuned during training. We employ early\\nstopping on the development set and report the av-\\neraged test set score across ﬁve runs with different',\n",
       " 'stopping on the development set and report the av-\\neraged test set score across ﬁve runs with different\\nrandom seeds.\\nELMo was added to the input of the lowest layer\\ntask biLSTM. As the CoNLL 2003 NER data set',\n",
       " 'ELMo was added to the input of the lowest layer\\ntask biLSTM. As the CoNLL 2003 NER data set\\nis relatively small, we found the best performance\\nby constraining the trainable layer weights to be\\neffectively constant by setting \\x15= 0:1with (1).',\n",
       " 'by constraining the trainable layer weights to be\\neffectively constant by setting \\x15= 0:1with (1).\\nTable 12 compares test set F 1scores of our\\nELMo enhanced biLSTM-CRF tagger with previ-\\nous results. Overall, the 92.22% F 1from our sys-',\n",
       " 'ELMo enhanced biLSTM-CRF tagger with previ-\\nous results. Overall, the 92.22% F 1from our sys-\\ntem establishes a new state-of-the-art. When com-\\npared to Peters et al. (2017), using representations\\nModel F1\\x06std.',\n",
       " 'pared to Peters et al. (2017), using representations\\nModel F1\\x06std.\\nCollobert et al. (2011)|89.59\\nLample et al. (2016) 90.94\\nMa and Hovy (2016) 91.2',\n",
       " 'Lample et al. (2016) 90.94\\nMa and Hovy (2016) 91.2\\nChiu and Nichols (2016)|;}91.62\\x060.33\\nPeters et al. (2017)}91.93\\x060.19\\nbiLSTM-CRF + ELMo 92.22\\x060.10',\n",
       " 'Peters et al. (2017)}91.93\\x060.19\\nbiLSTM-CRF + ELMo 92.22\\x060.10\\nTable 12: Test set F 1for CoNLL 2003 NER task. Mod-\\nels with|included gazetteers and those with}used\\nboth the train and development splits for training.',\n",
       " 'els with|included gazetteers and those with}used\\nboth the train and development splits for training.\\nModel Acc.\\nDMN (Kumar et al., 2016) 52.1\\nLSTM-CNN (Zhou et al., 2016) 52.4',\n",
       " 'DMN (Kumar et al., 2016) 52.1\\nLSTM-CNN (Zhou et al., 2016) 52.4\\nNTI (Munkhdalai and Yu, 2017) 53.1\\nBCN+Char+CoVe (McCann et al., 2017) 53.7\\nBCN+ELMo 54.7',\n",
       " 'BCN+Char+CoVe (McCann et al., 2017) 53.7\\nBCN+ELMo 54.7\\nTable 13: Test set accuracy for SST-5.\\nfrom all layers of the biLM provides a modest im-\\nprovement.',\n",
       " 'from all layers of the biLM provides a modest im-\\nprovement.\\nA.8 Sentiment classiﬁcation\\nWe use almost the same biattention classiﬁcation\\nnetwork architecture described in McCann et al.',\n",
       " 'We use almost the same biattention classiﬁcation\\nnetwork architecture described in McCann et al.\\n(2017), with the exception of replacing the ﬁnal\\nmaxout network with a simpler feedforward net-\\nwork composed of two ReLu layers with dropout.',\n",
       " 'maxout network with a simpler feedforward net-\\nwork composed of two ReLu layers with dropout.\\nA BCN model with a batch-normalized maxout\\nnetwork reached signiﬁcantly lower validation ac-\\ncuracies in our experiments, although there may',\n",
       " 'network reached signiﬁcantly lower validation ac-\\ncuracies in our experiments, although there may\\nbe discrepancies between our implementation and\\nthat of McCann et al. (2017). To match the CoVe\\ntraining setup, we only train on phrases that con-',\n",
       " 'that of McCann et al. (2017). To match the CoVe\\ntraining setup, we only train on phrases that con-\\ntain four or more tokens. We use 300-d hidden\\nstates for the biLSTM and optimize the model pa-\\nrameters with Adam (Kingma and Ba, 2015) us-',\n",
       " 'states for the biLSTM and optimize the model pa-\\nrameters with Adam (Kingma and Ba, 2015) us-\\ning a learning rate of 0.0001. The trainable biLM\\nlayer weights are regularized by \\x15= 0:001, and\\nwe add ELMo to both the input and output of the']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_whole_text(filepath):\n",
    "\treader = PdfReader(filepath)\n",
    "\twhole_text = [page.extract_text() for  page in reader.pages]\n",
    "\treturn(\"\\n\".join(whole_text))\n",
    "\n",
    "\n",
    "def get_text_chunks(whole_text, chunk_length, step): \n",
    "\tpara_list = whole_text.split(\"\\n\")\n",
    "\t# print(para_list)\n",
    "\tchunk_list = []\n",
    "\tfor i in range((len(para_list)-chunk_length +1)//step):\n",
    "\t\tchunk_list += [\"\\n\".join(para_list[i*step:i*step + chunk_length])]\n",
    "\treturn chunk_list\n",
    "\n",
    "\n",
    "text = get_whole_text(test_doc_path)\n",
    "get_text_chunks(text,5,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_docs/SWE_at_google_test_pdf.pdf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Software Engineering  at Google ',\n",
       " '31 Jan 2017 ',\n",
       " 'Fergus Henderson ',\n",
       " 'fergus@google.com',\n",
       " '> (work) or  <',\n",
       " 'fergus.henderson@gmail.com',\n",
       " '> (personal)',\n",
       " 'Abstract ',\n",
       " 'We catalog and describe Google’s key software engineering practices. ',\n",
       " 'Biography ',\n",
       " 'Fergus Henderson has been a software engineer at Google  for over 10 years. He started programming as a kid in 1979, and went on to academic research in programming language design and implementation. With his PhD supervisor, he co-founded a research group at the University of Melbourne that developed the programming language Mercury. He has been a program committee member for eight international conferences, and has released over 500,000 lines of open-source code. He was a former moderator of the Usenet newsgroup comp.std.c++ and was an officially accredited',\n",
       " '“Technical Expert” to the ISO C and C++ committees.',\n",
       " \"He has over 15 years of commercial software industry experience. At Google, he was one of the original developers of Blaze, a build tool now used across Google, and worked on the server-side software behind speech recognition and voice actions (before Siri!) and speech synthesis. He currently manages Google's text-to-speech engineering team, but still writes and reviews plenty of code. Software that he has written is installed on over a billion devices, and gets used over a billion times per day. \",\n",
       " 'Software Engineering at Google, by Fergus Henderson ',\n",
       " 'Contents',\n",
       " 'Abstract  Biography  Contents  1. Introduction  2. Software development  2.1. The Source Repository  2.2. The Build System  2.3. Code Review  2.4. Testing  2.5. Bug tracking  2.6. Programming languages  2.7. Debugging and Profiling tools  2.8. Release engineering  2.9. Launch approval  2.10. Post-mortems  2.11. Frequent rewrites  3. Project management  3.1. 20% time  3.2. Objectives and Key Results (OKRs)  3.3. Project approval  3.4. Corporate reorganizations  4. People management  4.1. Roles  4.2. Facilities  4.3. Training  4.4. Transfers  4.5. Performance appraisal and rewards  5. Conclusions  Acknowledgements  References ',\n",
       " '2  Software Engineering at Google, by Fergus Henderson ',\n",
       " '1. Introduction ',\n",
       " 'Google has been a phenomenally successful company',\n",
       " '. As well as the success of Google Search and AdWords, Google has delivered many other stand-out products, including Google Maps, Google News, Google Translate, Google speech recognition, Chrome, and Android. Google has also greatly enhanced and scaled many products that were acquired by purchasing smaller companies, such as YouTube, and has made significant contributions to a wide variety of open-source projects. And Google has demonstrated some amazing products that are yet to launch, such as self-driving cars.  There are many reasons for Google’s success, including enlightened leadership, great people, a high hiring bar, and the financial strength that comes from successfully taking advantage of an early lead in a very rapidly growing market. But one of these reasons is that',\n",
       " 'Google has developed excellent software engineering practices',\n",
       " ', which have helped it to succeed. These practices have evolved over time based on the accumulated and distilled wisdom of many of the most talented software engineers on the planet. We would like to share knowledge of our practices with the world, and to share some of the lessons that we have learned from our mistakes along the way. ',\n",
       " 'The aim of this paper is to catalogue and briefly describe Google’s key software engineering practices.',\n",
       " 'Other organizations and individuals can then compare and contrast these with their own software engineering practices, and consider whether to apply some of these practices themselves.  Many authors (e.g. [9], [10], [11]) have written books or articles analyzing Google’s success and history. But most of those have dealt mainly with business, management, and culture; only a fraction of those (e.g. [1, 2, 3, 4, 5, 6, 7, 13, 14, 16, 21]) have explored the software engineering side of things, and most explore only a single aspect; and none of them provide a brief written overview of software engineering practices at Google as a whole, as this paper aims to do. ',\n",
       " '2. Software development ',\n",
       " '2.1. The Source Repository ',\n",
       " 'Most of Google’s code is stored in a single unified source-code repository, and is accessible to all software engineers at Google',\n",
       " '. There are some notable exceptions, particularly the two large open-source projects Chrome and Android, which use separate open-source repositories, and some high-value or security-critical pieces of code for which read access is locked down more tightly. But most Google projects share the same repository. As of January 2015, this 86 terabyte repository contained a billion files, including over 9 million source',\n",
       " '3  Software Engineering at Google, by Fergus Henderson ',\n",
       " 'code files containing a total of',\n",
       " '2 billion lines of source code',\n",
       " ', with a history of 35 million commits and a change rate of 40 thousand commits per work day [18]. Write access to the repository is controlled: only the listed owners of each subtree of the repository can approve changes to that subtree. But generally any engineer can access any piece of code, can check it out and build it, can make local modifications, can test them, and can send changes for review by the code owners, and if an owner approves, can check in (commit) those changes. Culturally, engineers are encouraged to fix anything that they see is broken and know how to fix, regardless of project boundaries. This empowers engineers and leads to higher-quality infrastructure that better meets the needs of those using it. ',\n",
       " 'Almost all development occurs at the “head” of the repository',\n",
       " ', not on branches. This helps identify integration problems early and minimizes the amount of merging work needed. It also makes it much easier and faster to push out security fixes. ',\n",
       " 'Automated systems run tests frequently,',\n",
       " ' often after every change to any file in the transitive dependencies of the test, although this is not always feasible. These systems automatically notify the author and reviewers of any change for which the tests failed, typically within a few minutes. Most teams make the current status of their build very conspicuous by installing prominent displays or even sculptures with color-coded lights (green for building successfully and all tests passing, red for some tests failing, black for broken build). This helps to focus engineers’ attention on keeping the build green. Most larger teams also have a “build cop” who is responsible for ensuring that the tests continue to pass at head, by working with the authors of the offending changes to quickly fix any problems or to roll back the offending change. (The build cop role is typically rotated among the team or among its more experienced members.) This focus on keeping the build green makes development at head practical, even for very large teams. ',\n",
       " 'Code ownership.',\n",
       " 'Each subtree of the repository can have a file listing the user ids of the “owners” of that subtree. Subdirectories also inherit owners from their parent directories, although that can be optionally suppressed. The owners of each subtree control write access to that subtree, as described in the code review section below. Each subtree is required to have at least two owners, although typically there are more, especially in geographically distributed teams. It is common for the whole team to be listed in the owners file. Changes to a subtree can be made by anyone at Google, not just the owners, but must be approved by an owner. This ensures that every change is reviewed by an engineer who understands the software being modified.  For more on the source code repository at Google, see [17, 18, 21]; and for how another large company deals with the same challenge, see [19]. ',\n",
       " '2.2. The Build System ',\n",
       " 'Google uses a distributed build system known as Blaze, which is responsible for compiling and',\n",
       " '4  Software Engineering at Google, by Fergus Henderson ',\n",
       " 'linking software and for running tests. It provides standard commands for building and testing software that work across the whole repository. These standard commands and the highly optimized implementation mean that',\n",
       " 'it is typically very simple and quick for any Google engineer to build and test any software in the repository',\n",
       " '. This consistency is a key enabler which helps to make it practical for engineers to make changes across project boundaries.  Programmers write “BUILD” files that Blaze uses to determine how to build their software. Build entities such as libraries, programs, and tests are declared using fairly high-level',\n",
       " 'declarative build specifications',\n",
       " ' that specify, for each entity, its name, its source files, and the libraries or other build entities that it depends on. These build specifications are comprised of declarations called “build rules” that each specify high-level concepts like “here is a C++ library with these source files which depends on these other libraries”, and it is up to the build system to map each build rule to a set of build steps, e.g. steps for compiling each source file and steps for linking, and for determining which compiler and compilation flags to use.  In some cases, notably Go programs, build files can be generated (and updated) automatically, since the dependency information in the BUILD files is (often) an abstraction of the dependency information in the source files. But they are nevertheless checked in to the repository. This ensures that the build system can quickly determine dependencies by analyzing only the build files rather than the source files, and it avoids excessive coupling between the build system and compilers or analysis tools for the many different programming languages supported.  The build system’s implementation uses Google’s distributed computing infrastructure. The work of each build is typically',\n",
       " 'distributed across hundreds or even thousands of machines',\n",
       " '. This makes it possible to build extremely large programs quickly or to run thousands of tests in parallel. ',\n",
       " 'Individual build steps must be “hermetic”: they depend only on their declared inputs.',\n",
       " 'Enforcing that all dependencies be correctly declared is a consequence of distributing the build: only the declared inputs are sent to the machine on which the build step is run. As a result the build system can be relied on to know the true dependencies. Even the compilers that the build system invokes are treated as inputs. ',\n",
       " 'Individual build steps are deterministic.',\n",
       " ' As a consequence, the build system can cache build results. Software engineers can sync their workspace back to an old change number and can rebuild and will get exactly the same binary. Furthermore, this cache can be safely shared between different users. (To make this work properly, we had to eliminate non-determinism in the tools invoked by the build, for example by scrubbing out timestamps in the generated output files.) ',\n",
       " 'The build system is reliable.',\n",
       " 'The build system tracks dependencies on changes to the build rules themselves, and knows to rebuild targets if the action to produce them changed, even if the inputs to that action didn’t, for example when only the compiler options changed. It also',\n",
       " '5  Software Engineering at Google, by Fergus Henderson ',\n",
       " 'deals properly with interrupting the build part way, or modifying source files during the build: in such cases, you need only rerun the build command. There is never any need to run the equivalent of “make clean”.  ',\n",
       " 'Build results are cached “in the cloud”',\n",
       " '. This includes intermediate results. If another build request needs the same results, the build system will automatically reuse them rather than rebuilding, even if the request comes from a different user. ',\n",
       " 'Incremental rebuilds are fast.',\n",
       " 'The build system stays resident in memory so that for rebuilds it can incrementally analyze just the files that have changed since the last build. ',\n",
       " 'Presubmit checks.',\n",
       " 'Google has tools for automatically running a suite of tests when initiating a code review and/or preparing to commit a change to the repository. Each subtree of the repository can contain a configuration file which determines which tests to run, and whether to run them at code review time, or immediately before submitting, or both. The tests can be either synchronous, i.e. run before sending the change for review and/or before committing the change to the repository (good for fast-running tests); or asynchronous, with the results emailed to the review discussion thread. [The review thread is the email thread on which the code review takes place; all the information in that thread is also displayed in the web-based code review tool.] ',\n",
       " '2.3. Code Review ',\n",
       " 'Google has built excellent web-based code review tools',\n",
       " ', integrated with email, that allow authors to request a review, and allows reviewers to view side-by-side diffs (with nice color coding) and comment on them. When the author of a change initiates a code review, the reviewers are notified by e-mail, with a link to the web review tool’s page for that change. Email notifications are sent when reviewers submit their review comments. In addition, automated tools can send notifications, containing for example the results of automated tests or the findings of static analysis tools. ',\n",
       " 'All changes to the main source code repository MUST be reviewed by at least one other engineer.',\n",
       " 'In addition, if the author of a change is not one of the owners of the files being modified, then at least one of the owners must review and approve the change.  In exceptional cases, an owner of a subtree can check in (commit) an urgent change to that subtree',\n",
       " ' it is reviewed, but a reviewer must still be named, and the change author and reviewer will get automatically nagged about it until the change has been reviewed and approved. In such cases, any modifications needed to address review comments must be done in a separate change, since the original change will have already been committed.  Google has tools for automatically suggesting reviewer(s) for a given change, by looking at the ownership and authorship of the code being modified, the history of recent reviewers, and the',\n",
       " '6  Software Engineering at Google, by Fergus Henderson ',\n",
       " 'number of pending code reviews for each potential reviewer. At least one of the owners of each subtree which a change affects must review and approve that change. But apart from that, the author is free to choose reviewer(s) as they see fit.  One potential issue with code review is that if the reviewers are too slow to respond or are overly reluctant to approve changes, this could potentially slow down development. The fact that the code author chooses their reviewers helps avoid such problems, allowing engineers to avoid reviewers that might be overly possessive about their code, or to send reviews for simple changes to less thorough reviewers and to send reviews for more complex changes to more experienced reviewers or to several reviewers. ',\n",
       " 'Code review discussions for each project are automatically copied to a mailing list designated by the project maintainers.',\n",
       " ' Anyone is free to comment on any change, regardless of whether they were named as a reviewer of that change, both before and after the change is committed. If a bug is discovered, it’s common to track down the change that introduced it and to comment on the original code review thread to point out the mistake so that the original author and reviewers are aware of it.  It is also possible to send code reviews to several reviewers and then to commit the change as soon as one of them has approved (provided either the author or the first responding reviewer is an owner, of course), before the other reviewers have commented, with any subsequent review comments being dealt with in follow-up changes. This can reduce the turnaround time for reviews.  In addition to the main section of the repository,',\n",
       " 'there is an “experimental” section of the repository where the normal code review requirements are not enforced',\n",
       " '. However, code running in production must be in the main section of the repository, and engineers are very strongly encouraged to develop code in the main section of the repository, rather than developing in experimental and then moving it to the main section, since code review is most effective when done as the code is developed rather than afterwards. In practice engineers often request code reviews even for code in experimental. ',\n",
       " 'Engineers are encouraged to keep each individual change small',\n",
       " ', with larger changes preferably broken into a series of smaller changes that a reviewer can easily review in one go. This also makes it easier for the author to respond to major changes suggested during the review of each piece; very large changes are often too rigid and resist reviewer-suggested changes. One way in which keeping changes small is encouraged is that the code review tools',\n",
       " 'label each code review with a description of the size of the change, with changes of 30-99 lines added/deleted/removed being labelled “medium-size” and with changes of above 300 lines being labelled with increasingly disparaging labels, e.g. “large” (300-999), “freakin huge”',\n",
       " ' This has changed somewhat in recent years. More recent versions of the code review tools no longer use the more disparaging labels for large CLs, but they are still labelled with their size, e.g. “S”, “M”, “L”, “XL”. ',\n",
       " '7  Software Engineering at Google, by Fergus Henderson ',\n",
       " '(1000-1999), etc. (However, in a typically Googly way, this is kept fun by replacing these familiar descriptions with amusing alternatives on a few days each year, such as talk-like-a-pirate day. :) ',\n",
       " '2.4. Testing ',\n",
       " 'Unit Testing is strongly encouraged and widely practiced at Google',\n",
       " '. All code used in production is expected to have unit tests, and the code review tool will highlight if source files are added without corresponding tests. Code reviewers usually require that any change which adds new functionality should also add new tests to cover the new functionality. Mocking frameworks (which allow construction of lightweight unit tests even for code with dependencies on heavyweight libraries) are quite popular.  Integration testing and regression testing are also widely practiced.  As discussed in',\n",
       " '\"Presubmit Checks\"  above, testing can be automatically enforced as part of the code review and commit process.  Google also has automated tools for measuring test coverage. The results are also integrated as an optional layer in the source code browser. ',\n",
       " 'Load testing prior to deployment',\n",
       " ' is also de rigueur at Google. Teams are expected to produce a table or graph showing how key metrics, particularly latency and error rate, vary with the rate of incoming requests. ',\n",
       " '2.5. Bug tracking ',\n",
       " 'Google uses a bug tracking system called Buganizer for tracking issues: bugs, feature requests, customer issues, and processes (such as releases or clean-up efforts). Bugs are categorized into hierarchical components and each component can have a default assignee and default email list to CC. When sending a source change for review, engineers are prompted to associate the change with a particular issue number.  It is common (though not universal) for teams at Google to regularly scan through open issues in their component(s), prioritizing them and where appropriate assigning them to particular engineers. Some teams have a particular individual responsible for bug triage, others do bug triage in their regular team meetings. Many teams at Google make use of labels on bugs to indicate whether bugs have been triaged, and which release(s) each bug is targeted to be fixed in. ',\n",
       " '2.6. Programming languages ',\n",
       " 'Software engineers at Google are strongly encouraged to program in one of five',\n",
       " '8  Software Engineering at Google, by Fergus Henderson ',\n",
       " 'officially-approved programming languages at Google:',\n",
       " 'C++, Java, Python, Go, or JavaScript',\n",
       " '. Minimizing the number of different programming languages used reduces obstacles to code reuse and programmer collaboration.  There are also Google',\n",
       " 'style guides',\n",
       " ' for each language, to ensure that code all across the company is written with similar style, layout, naming conventions, etc. In addition there is a company-wide',\n",
       " 'readability',\n",
       " ' training process, whereby experienced engineers who care about code readability train other engineers in how to write readable, idiomatic code in a particular language, by reviewing a substantial change or series of changes until the reviewer is satisfied that the author knows how to write readable code in that language. Each change that adds non-trivial new code in a particular language must be approved by someone who has passed this “readability” training process in that language.  In addition to these five languages, many',\n",
       " 'specialized domain-specific languages',\n",
       " ' are used for particular purposes (e.g. the build language used for specifying build targets and their dependencies).  Interoperation between these different programming languages is done mainly using',\n",
       " 'Protocol Buffers',\n",
       " '. Protocol Buffers is a way of encoding structured data in an efficient yet extensible way. It includes a domain-specific language for specifying structured data, together with a compiler that takes in such descriptions and generates code in C++, Java, Python, for constructing, accessing, serializing, and deserializing these objects. Google’s version of Protocol Buffers is integrated with Google’s RPC libraries, enabling simple cross-language RPCs, with serialization and deserialization of requests and responses handled automatically by the RPC framework. ',\n",
       " 'Commonality of process',\n",
       " ' is a key to making development easy even with an enormous code base and a diversity of languages: there is a single set of commands to perform all the usual software engineering tasks (such as check out, edit, build, test, review, commit, file bug report, etc.) and the same commands can be used no matter what project or language. Developers don’t need to learn a new development process just because the code that they are editing happens to be part of a different project or written in a different language. ',\n",
       " '2.7. Debugging and Profiling tools ',\n",
       " 'Google servers are linked with libraries that provide a number of tools for debugging running servers. In case of a server crash, a signal handler will automatically dump a stack trace to a log file, as well as saving the core file. If the crash was due to running out of heap memory, the server will dump stack traces of the allocation sites of a sampled subset of the live heap objects. There are also web interfaces for debugging that allow examining incoming and outgoing RPCs (including timing, error rates, rate limiting, etc.), changing command-line flag values (e.g. to increase logging verbosity for a particular module), resource consumption, profiling, and more.',\n",
       " '9  Software Engineering at Google, by Fergus Henderson ',\n",
       " 'These tools greatly increase the overall ease of debugging to the point where it is rare to fire up a traditional debugger such as gdb. ',\n",
       " '2.8. Release engineering ',\n",
       " 'A few teams have dedicated release engineers, but for most teams at Google, the release engineering work is done by regular software engineers. ',\n",
       " 'Releases are done frequently',\n",
       " ' for most software; weekly or fortnightly releases are a common goal, and some teams even release daily. This is made possible by',\n",
       " 'automating most of the normal release engineering tasks',\n",
       " '. Releasing frequently helps to keep engineers motivated (it’s harder to get excited about something if it won’t be released until many months or even years into the future) and increases overall velocity by allowing more iterations, and thus more opportunities for feedback and more chances to respond to feedback, in a given time.  A release typically starts in a fresh workspace, by syncing to the change number of the latest “green” build (i.e. the last change for which all the automatic tests passed), and making a release branch. The release engineer can select additional changes to be “cherry-picked”, i.e. merged from the main branch onto the release branch. Then the software will be rebuilt from scratch and the tests are run. If any tests fail, additional changes are made to fix the failures and those additional changes are cherry-picked onto the release branch, after which the software will be rebuilt and the tests rerun. When the tests all pass, the built executable(s) and data file(s) are packaged up. All of these steps are automated so that the release engineer need only run some simple commands, or even just select some entries on a menu-driven UI, and choose which changes (if any) to cherry pick.  Once a candidate build has been packaged up, it is typically loaded onto a “',\n",
       " 'staging',\n",
       " '” server for further ',\n",
       " 'integration testing by small set of users',\n",
       " ' (sometimes just the development team).  A useful technique involves sending a copy of (a subset of) the requests from production traffic to the staging server, but also sending those same requests to the current production servers for actual processing. The responses from the staging server are discarded, and the responses from the live production servers are sent back to the users. This helps ensure that any issues that might cause serious problems (e.g. server crashes) can be detected before putting the server into production.  The next step is to usually roll out to one or more “',\n",
       " '” servers that are',\n",
       " 'processing a subset of the live production traffic.',\n",
       " ' Unlike the “staging” servers, these are processing and responding to real users.  Finally the release can be rolled out to all servers in all data centers. For very high-traffic, high-reliability services, this is done with a',\n",
       " 'gradual roll-out',\n",
       " ' over a period of a couple of days, to help reduce the impact of any outages due to newly introduced bugs not caught by any of the',\n",
       " '10  Software Engineering at Google, by Fergus Henderson ',\n",
       " 'previous steps.  For more information on release engineering at Google, see chapter 8 of the SRE book [7]. See also [15]. ',\n",
       " '2.9. Launch approval ',\n",
       " 'The launch of any user-visible change or significant design change requires approvals from a number of people outside of the core engineering team that implements the change. In particular approvals (often subject to detailed review) are required to ensure that code complies with legal requirements, privacy requirements, security requirements, reliability requirements (e.g. having appropriate automatic monitoring to detect server outages and automatically notify the appropriate engineers), business requirements, and so forth.  The launch process is also designed to ensure that appropriate people within the company are notified whenever any significant new product or feature launches. ',\n",
       " 'Google has an internal launch approval tool that is used to track the required reviews and approvals and ensure compliance with the defined launch processes for each product.',\n",
       " ' This tool is easily customizable, so that different products or product areas can have different sets of required reviews and approvals.  For more information about launch processes, see chapter 27 of the SRE book [7]. ',\n",
       " '2.10. Post-mortems ',\n",
       " 'Whenever there is a significant outage of any of our production systems, or similar mishap, the people involved are required to write a post-mortem document. This document describes the incident, including title, summary, impact, timeline, root cause(s), what worked/what didn’t, and action items.',\n",
       " 'The focus is on the problems, and how to avoid them in future, not on the people or apportioning blame.',\n",
       " 'The impact section tries to quantify the effect of the incident, in terms of duration of outage, number of lost queries (or failed RPCs, etc.), and revenue. The timeline section gives a timeline of the events leading up to the outage and the steps taken to diagnose and rectify it. The what worked/what didn’t section describes the lessons learnt -- which practices helped to quickly detect and resolve the issue, what went wrong, and what concrete actions (preferably filed as bugs assigned to specific people) can be take to reduce the likelihood and/or severity of similar problems in future.  For more information on post-mortem culture at Google, see chapter 15 of the SRE book [7]. ',\n",
       " '2.11. Frequent rewrites ',\n",
       " 'Most software at Google gets rewritten every few years. ',\n",
       " '11  Software Engineering at Google, by Fergus Henderson ',\n",
       " 'This may seem incredibly costly. Indeed, it does consume a large fraction of Google’s resources. However, it also has some crucial benefits that are key to Google’s agility and long-term success. In a period of a few years, it is typical for the requirements for a product to change significantly, as the software environment and other technology around it change, and as changes in technology or in the marketplace affect user needs, desires, and expectations. Software that is a few years old was designed around an older set of requirements and is typically not designed in a way that is optimal for current requirements. Furthermore, it has typically accumulated a lot of complexity. Rewriting code cuts away all the unnecessary accumulated complexity that was addressing requirements which are no longer so important. In addition, rewriting code is a way of transferring knowledge and a sense of ownership to newer team members. This sense of ownership is crucial for productivity: engineers naturally put more effort into developing features and fixing problems in code that they feel is “theirs”. Frequent rewrites also encourage mobility of engineers between different projects which helps to encourage cross-pollination of ideas. Frequent rewrites also help to ensure that code is written using modern technology and methodology. ',\n",
       " '3. Project management ',\n",
       " '3.1. 20% time ',\n",
       " 'Engineers are permitted to spend up to 20% of their time working on any project of their choice, without needing approval from their manager or anyone else.',\n",
       " 'This trust in engineers is extremely valuable, for several reasons. Firstly, it allows anyone with a good idea, even if it is an idea that others would not immediately recognize as being worthwhile, to have sufficient time to develop a prototype, demo, or presentation to show the value of their idea. Secondly, it provides management with visibility into activity that might otherwise be hidden. In other companies that don’t have an official policy of allowing 20% time, engineers sometimes work on “skunkwork” projects without informing management. It’s much better if engineers can be open about such projects, describing their work on such projects in their regular status updates, even in cases where their management may not agree on the value of the project. Having a company-wide official policy and a culture that supports it makes this possible. Thirdly, by allowing engineers to spend a small portion of their time working on more fun stuff, it keeps engineers motivated and excited by what they do, and stops them getting burnt out, which can easily happen if they feel compelled to spend 100% of their time working on more tedious tasks. The difference in productivity between engaged, motivated engineers and burnt out engineers is a',\n",
       " ' more than 20%. Fourthly, it encourages a culture of innovation. Seeing other engineers working on fun experimental 20% projects encourages everyone to do the same. ',\n",
       " '3.2. Objectives and Key Results (OKRs) ',\n",
       " '12  Software Engineering at Google, by Fergus Henderson ',\n",
       " 'Individuals and teams at Google are required to explicitly document their goals and to assess their progress towards these goals.',\n",
       " ' Teams set quarterly and annual objectives, with measurable key results that show progress towards these objectives. This is done at every level of the company, going all the way up to defining goals for the whole company. Goals for individuals and small teams should align with the higher-level goals for the broader teams that they are part of and with the overall company goals. At the end of each quarter, progress towards the measurable key results is recorded and each objective is given a score from 0.0 (no progress) to 1.0 (100% completion). OKRs and OKR scores are normally made visible across Google (with occasional exceptions for especially sensitive information such as highly confidential projects), but they',\n",
       " 'used directly as input to an individual’s performance appraisal.  OKRs should be set high: the desired target overall average score is 65%, meaning that a team is encouraged to set as goals about 50% more tasks than they are likely to actually accomplish. If a team scores significantly higher than that, they are encouraged to set more ambitious OKRs for the following quarter (and conversely if they score significantly lower than that, they are encouraged to set their OKRs more conservatively the next quarter).  OKRs provide a key mechanism for communicating what each part of the company is working on, and for encouraging good performance from employees via social incentives… engineers know that their team will have a meeting where the OKRs will be scored, and have a natural drive to try to score well, even though OKRs have no direct impact on performance appraisals or compensation. Defining key results that are objective and measurable helps ensure that this human drive to perform well is channelled to doing things that have real concrete measurable impact on progress towards shared objectives. ',\n",
       " '3.3. Project approval ',\n",
       " 'Although there is a well-defined process for launch approvals, Google does not have a well-defined process for project approval or cancellation. Despite having been at Google for over 10 years, and now having become a manager myself, I still don’t fully understand how such decisions are made. In part this is because the approach to this is not uniform across the company. Managers at every level are responsible and accountable for what projects their teams work on, and exercise their discretion as they see fit. In some cases, this means that such decisions are made in a quite bottom-up fashion, with engineers being given freedom to choose which projects to work on, within their team’s scope. In other cases, such decisions are made in a much more top-down fashion, with executives or managers making decisions about which projects will go ahead, which will get additional resources, and which will get cancelled. ',\n",
       " '3.4. Corporate reorganizations ',\n",
       " 'Occasionally an executive decision is made to cancel a large project, and then the many engineers who had been working on that project may have to find new projects on new teams.',\n",
       " '13  Software Engineering at Google, by Fergus Henderson ',\n",
       " 'Similarly there have been occasional “defragmentation” efforts, where projects that are split across multiple geographic locations are consolidated into a smaller number of locations, with engineers in some locations being required to change team and/or project in order to achieve this. In such cases, engineers are generally given freedom to choose their new team and role from within the positions available in their geographic location, or in the case of defragmentation, they may also be given the option of staying on the same team and project by moving to a different location.  In addition, other kinds of corporate reorganizations, such as merging or splitting teams and changes in reporting chains, seem to be fairly frequent occurrences, although I don’t know how Google compares with other large companies on that. In a large, technology-driven organization, somewhat frequent reorganization may be necessary to avoid organizational inefficiencies as the technology and requirements change. ',\n",
       " '4. People management ',\n",
       " '4.1. Roles ',\n",
       " 'As we’ll explain in more detail below, Google separates the engineering and management career progression ladders, separates the tech lead role from management, embeds research within engineering, and supports engineers with product managers, project managers, and site reliability engineers (SREs). It seems likely that at least some of these practices are important to sustaining the culture of innovation that has developed at Google.  Google has a small number of different roles within engineering. Within each role, there is a career progression possible, with a sequence of levels, and the possibility of promotion (with associated improvement to compensation, e.g. salary) to recognize performance at the next level.  The main roles are these: ',\n",
       " '● Engineering Manager ',\n",
       " 'This is the only people management role in this list. Individuals in other roles such as Software Engineer',\n",
       " 'manage people, but Engineering Managers',\n",
       " 'manage people. Engineering Managers are often former Software Engineers, and invariably have considerable technical expertise, as well as people skills. ',\n",
       " 'There is a distinction between technical leadership and people management.',\n",
       " 'Engineering Managers do not necessarily lead projects; projects are led by a Tech Lead, who can be an Engineering Manager, but who is more often a Software Engineer. A project’s Tech Lead has the final say for technical decisions in that project. ',\n",
       " '14  Software Engineering at Google, by Fergus Henderson ',\n",
       " 'Managers are responsible for selecting Tech Leads, and for the performance of their teams. They perform coaching and assisting with career development, do performance evaluation (using input from peer feedback, see below), and are responsible for some aspects of compensation. They are also responsible for some parts of the hiring process.  Engineering Managers normally directly manage anywhere between 3 and 30 people, although 8 to 12 is most common. ',\n",
       " '● Software Engineer (SWE) ',\n",
       " 'Most people doing software development work have this role. The hiring bar for software engineers at Google is very high; by hiring only exceptionally good software engineers, a lot of the software problems that plague other organizations are avoided or minimized. ',\n",
       " 'Google has separate career progression sequences for engineering and management',\n",
       " '. Although it is possible for a Software Engineer to manage people, or to transfer to the Engineering Manager role, managing people is',\n",
       " 'a requirement for promotion, even at the highest levels. At the higher levels, showing leadership is required, but that can come in many forms. For example creating great software that has a huge impact or is used by very many other engineers is sufficient. This is important, because it means that people who have great technical skills but lack the desire or skills to manage people still have a good career progression path that does not require them to take a management track. This avoids the problem that some organizations suffer where people end up in management positions for reasons of career advancement but neglect the people management of the people in their team.  ●',\n",
       " 'Research Scientist ',\n",
       " 'The hiring criteria for this role are very strict, and the bar is extremely high, requiring demonstrated exceptional research ability evidenced by a great publication record *and* ability to write code. Many very talented people in academia who would be able to qualify for a Software Engineer role would not qualify for a Research Scientist role at Google; most of the people with PhDs at Google are Software Engineers rather than Research Scientists. Research scientists are evaluated on their research contributions, including their publications, but apart from that and the different title, there is not really that much difference between the Software Engineer and Research Scientist role at Google. Both can do original research and publish papers, both can develop new product ideas and new technologies, and both can and do write code and develop products. Research Scientists at Google usually work alongside Software Engineers, in',\n",
       " '15  Software Engineering at Google, by Fergus Henderson ',\n",
       " 'the same teams and working on the same products or the same research. This practice of embedding research within engineering contributes greatly to the ease with which new research can be incorporated into shipping products.  ●',\n",
       " 'Site Reliability Engineer (SRE) ',\n",
       " 'The maintenance of operational systems is done by software engineering teams, rather than traditional sysadmin types, but the hiring requirements for SREs are slightly different than the requirements for the Software Engineer position (software engineering skills requirements can be slightly lower, if compensated for by expertise in other skills such as networking or unix system internals). The nature and purpose of the SRE role is explained very well and in detail in the SRE book [7], so we won’t discuss it further here.  ●',\n",
       " 'Product Manager ',\n",
       " 'Product Managers are responsible for the management of a product; as advocates for the product users, they coordinate the work of software engineers, evangelizing features of importance to those users, coordinating with other teams, tracking bugs and schedules, and ensuring that everything needed is in place to produce a high quality product. Product Managers usually do NOT write code themselves, but work with software engineers to ensure that the right code gets written. ',\n",
       " '● Program Manager / Technical Program Manager ',\n",
       " 'Program Managers have a role that is broadly similar to Product Manager, but rather than managing a product, they manage projects, processes, or operations (e.g. data collection). Technical Program Managers are similar, but also require specific technical expertise relating to their work, e.g. linguistics for dealing with speech data.  The ratio of Software Engineers to Product Managers and Program Managers varies across the organization, but is generally high, e.g. in the range 4:1 to 30:1. ',\n",
       " '4.2. Facilities ',\n",
       " 'Google is famous for its fun facilities, with features like slides, ball pits, and games rooms. That helps attract and retain good talent. Google’s excellent cafes, which are free to employees, provide that function too, and also subtly encourage Googlers to stay in the office; hunger is never a reason to leave. The frequent placement of “microkitchens” where employees can grab snacks and drinks serves the same function too, but also acts as an important source of informal idea exchange, as many conversations start up there. Gyms, sports, and on-site massage help keep employees fit, healthy, and happy, which improves productivity and retention. ',\n",
       " '16  Software Engineering at Google, by Fergus Henderson ',\n",
       " 'The seating at Google is open-plan, and often fairly dense. While controversial [20], this encourages communication, sometimes at the expense of individual concentration, and is economical.  Employees are assigned an individual seat, but seats are re-assigned fairly frequently (e.g. every 6-12 months, often as a consequence of the organization expanding), with seating chosen by managers to facilitate and encourage communication, which is always easier between adjacent or nearly adjacent individuals.  Google’s facilities all have meeting rooms fitted with state-of-the-art video conference facilities, where connecting to the other party for a prescheduled calendar invite is just a single tap on the screen. ',\n",
       " '4.3. Training ',\n",
       " 'Google encourages employee education in many ways:  ● New Googlers (“Nooglers”) have a mandatory initial training course.  ● Technical staff (SWEs and research scientists) start by doing “Codelabs”: short online training courses in individual technologies, with coding exercises.  ● Google offers employees a variety of online and in-person training courses.  ● Google also offers support for studying at external institutions.  In addition, each Noogler is usually appointed an official “Mentor” and a separate “Buddy” to help get them up to speed. Unofficial mentoring also occurs via regular meetings with their manager, team meetings, code reviews, design reviews, and informal processes. ',\n",
       " '4.4. Transfers ',\n",
       " 'Transfers between different parts of the company are encouraged',\n",
       " ', to help spread knowledge and technology across the organization and improve cross-organization communication.',\n",
       " 'Transfers between projects and/or offices are allowed for employees in good standing after 12 months in a position. Software engineers are also encouraged to do temporary assignments in other parts of the organization, e.g. a six-month “rotation” (temporary assignment) in SRE (Site Reliability Engineering). ',\n",
       " '4.5. Performance appraisal and rewards ',\n",
       " 'Feedback is strongly encouraged at Google. Engineers can give each other explicit positive feedback via “peer bonuses” and “kudos”. Any employee can nominate any other employee for a “peer bonus” -- a cash bonus of $100 -- up to twice per year, for going beyond the normal call of duty, just by filling in a web form to describe the reason. Team-mates are also typically notified when a peer bonus is awarded. Employees can also give “kudos”, formalized statements of praise which provide explicit social recognition for good work, but with no financial',\n",
       " '17  Software Engineering at Google, by Fergus Henderson ',\n",
       " 'reward; for “kudos” there is no requirement that the work be beyond the normal call of duty, and no limit on the number of times that they can be bestowed.  Managers can also award bonuses, including spot bonuses, e.g. for project completion. And as with many companies, Google employees get annual performance bonuses and equity awards based on their performance.  Google has a very careful and detailed promotion process, which involves nomination by self or manager, self-review, peer reviews, manager appraisals; the actual decisions are then made by promotion committees based on that input, and the results can be subject to further review by promotion appeals committees. Ensuring that the right people get promoted is critical to maintaining the right incentives for employees.  Poor performance, on the other hand, is handled with manager feedback, and if necessary with performance improvement plans, which involve setting very explicit concrete performance targets and assessing progress towards those targets. If that fails, termination for poor performance is possible, but in practice this is ',\n",
       " 'extremely',\n",
       " ' rare at Google.  Manager performance is assessed with feedback surveys; every employee is asked to fill in an survey about the performance of their manager twice a year, and the results are anonymized and aggregated and then made available to managers. This kind of upward feedback is very important for maintaining and improving the quality of management throughout the organization.',\n",
       " '5. Conclusions ',\n",
       " 'We have briefly described most of the key software engineering practices used at Google. Of course Google is now a large and diverse organization, and some parts of the organization have different practices. But the practices described here are generally followed by most teams at Google.  With so many different software engineering practices involved, and with so many other reasons for Google’s success that are not related to our software engineering practices, it is extremely difficult to give any quantitative or objective evidence connecting individual practices with improved outcomes. However, these practices are the ones that have stood the test of time at Google, where they have been subject to the collective subjective judgement of many thousands of excellent software engineers.  For those in other organizations who are advocating for the use of a particular practice that happens to be described in this paper, perhaps it will help to say “it’s good enough for Google”. ',\n",
       " '18  Software Engineering at Google, by Fergus Henderson ',\n",
       " 'Acknowledgements ',\n",
       " 'Special thanks to Alan Donovan for his extremely detailed and constructive feedback, and thanks also to Y',\n",
       " 'aroslav Volovich, Urs',\n",
       " ' Hölzle, Brian Strope, Alexander Gutkin, Alex Gruenstein and Hameed Husaini for their very helpful comments on earlier drafts of this paper. ',\n",
       " 'References ',\n",
       " 'Build in the Cloud: Accessing Source Code',\n",
       " ', Nathan York,  http://google-engtools.blogspot.com/2011/06/build-in-cloud-accessing-source-code.html [2] ',\n",
       " 'Build in the Cloud: How the Build System works',\n",
       " 'Christian Kemper,  http://google-engtools.blogspot.com/2011/08/build-in-cloud-how-build-system-works.htm [3] ',\n",
       " 'Build in the Cloud: Distributing Build Steps, ',\n",
       " 'Nathan York http://google-engtools.blogspot.com/2011/09/build-in-cloud-distributing-build-steps.html [4] ',\n",
       " 'Build in the Cloud: Distributing Build Outputs, ',\n",
       " 'Milos Besta, Yevgeniy Miretskiy and Jeff Cox http://google-engtools.blogspot.com/2011/10/build-in-cloud-distributing-build.html [5] ',\n",
       " 'Testing at the speed and scale of Google',\n",
       " ', Pooja Gupta, Mark Ivey, and John Penix, Google  engineering tools blog, June 2011.  http://google-engtools.blogspot.com/2011/06/testing-at-speed-and-scale-of-google.html [6] ',\n",
       " 'Building Software at Google Scale Tech Talk, ',\n",
       " 'Michael Barnathan, Greg Estren, Pepper  Lebeck-Jone,',\n",
       " 'Google tech talk.  http://www.youtube.com/watch?v=2qv3fcXW1mg [7] ',\n",
       " 'Site Reliability Engineering',\n",
       " \", Betsy Beyer, Chris Jones, Jennifer Petoff, Niall Richard Murphy,  O'Reilly Media, April 2016, ISBN 978-1-4919-2909-4.  https://landing.google.com/sre/book.html [8] \",\n",
       " 'How Google Works,',\n",
       " ' Eric Schmidt, Jonathan Rosenberg.  http://www.howgoogleworks.net [9] ',\n",
       " 'What would Google Do?: Reverse-Engineering the Fastest Growing Company in the History  of the World',\n",
       " ', Jeff Jarvis, Harper Business, 2011.  https://books.google.co.uk/books/about/What_Would_Google_Do.html?id=GvkEcAAACAAJ&re dir_esc=y [10] ',\n",
       " 'The Search: How Google and Its Rivals Rewrote the Rules of Business and Transformed  Our Culture',\n",
       " ', John Battelle, 8 September 2005.  https://books.google.co.uk/books/about/The_Search.html?id=4MY8PgAACAAJ&redir_esc=y [11] ',\n",
       " 'The Google Story',\n",
       " ', David A. Vise, Pan Books, 2008.  http://www.thegooglestory.com/ [12] ',\n",
       " 'Searching for Build Debt: Experiences Managing Technical Debt at Google,',\n",
       " ' J. David  Morgenthaler, Misha Gridnev, Raluca Sauciuc, and Sanjay Bhansali.  http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37755.pdf',\n",
       " '19  Software Engineering at Google, by Fergus Henderson ',\n",
       " 'Development at the speed and scale of Google',\n",
       " ', A. Kumar, December 2010, presentation,  QCon.  http: //www.infoq.com/presentations/Development-at-Google [14] ',\n",
       " 'How Google Tests Software',\n",
       " ', J. A. Whittaker, J. Arbon, and J. Carollo, Addison-Wesley,  2012.   [15] ',\n",
       " 'Release Engineering Practices and Pitfalls',\n",
       " ', H. K. Wright and D. E. Perry, in ',\n",
       " 'Proceedings of  the 34th International Conference on Software Engineering (ICSE ’12)',\n",
       " ', IEEE, 2012, pp.  1281–1284.  http://www.hyrumwright.org/papers/icse2012.pdf [16] ',\n",
       " 'Large-Scale Automated Refactoring Using ClangMR',\n",
       " ', H. K. Wright, D. Jasper, M. Klimek, C.  Carruth, Z. Wan, in ',\n",
       " 'Proceedings of the 29th International Conference on Software Maintenance  (ICSM ’13)',\n",
       " ', IEEE, 2013, pp. 548–551.  [17] ',\n",
       " 'Why Google Stores Billions of Lines of Code in a Single Repository',\n",
       " ', Rachel Potvin,  presentation.  https://www.youtube.com/watch?v=W71BTkUbdqE [18] ',\n",
       " 'The Motivation for a Monolithic Codebase',\n",
       " ', Rachel Potvin, Josh Levenberg, to be published  in Communications of the ACM, July 2016.  [19] ',\n",
       " 'Scaling Mercurial at Facebook, ',\n",
       " 'Durham Goode, Siddharth P. Agarwa, Facebook blog post,  January 7th, 2014.  https://code.facebook.com/posts/218678814984400/scaling-mercurial-at-facebook/ [20] ',\n",
       " 'Why We (Still) Believe In Private Offices',\n",
       " ', David Fullerton, Stack Overflow blog post,  January 16th, 2015.  https://blog.stackoverflow.com/2015/01/why-we-still-believe-in-private-offices/ [21] ',\n",
       " 'Continuous Integration at Google Scale',\n",
       " ', John Micco, presentation, EclipseCon, 2013.  http://eclipsecon.org/2013/sites/eclipsecon.org.2013/files/2013-03-24%20Continuous%20Integr ation%20at%20Google%20Scale.pdf']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pdf_util\n",
    "from importlib import reload \n",
    "pdf_util = reload(pdf_util)\n",
    "\n",
    "\n",
    "test_docname = \"Paper2-ElMo.pdf\"\n",
    "test_docname = \"SWE_at_google_test_pdf.pdf\"\n",
    "# test_docname = \"Rhet_Ops.pdf\"\n",
    "\n",
    "test_doc_path = path.join(\"test_docs\",test_docname)\n",
    "print(test_doc_path)\n",
    "\n",
    "pdf_util.segment_document(test_doc_path,combine_blocks=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('marvin')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "6caab33d2a4b22a4d8f9bb910c5b20b3cbbc9401a5a2a486667699d93b7bac47"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
